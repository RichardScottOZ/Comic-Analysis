# Model Debugging Findings: ClosureLiteSimple Image Embeddings

## Problem: Non-Discriminative Image Embeddings (Continued)

**Initial Symptom:** When using the "Image Search" functionality, uploading different images consistently yielded the same search results with low similarity scores (e.g., max 0.815), even for images known to be in the dataset. This indicated that the `query_embedding` generated from the uploaded image was not unique or discriminative.

**Initial Debug Output (from `cosine_similarity_search`):**
```
DEBUG: Image embedding (E_page) shape: torch.Size([1, 384])
DEBUG: Image embedding (E_page) sample (first 5 elements): [ 0.7097523  -0.1343099  -0.37703553 -0.03819691 -0.18046191]
DEBUG: cosine_similarity_search - Similarities max: 0.815126895904541, min: -0.7508172392845154, mean: -0.05537611246109009
```
Crucially, these values remained constant across different image uploads.

## Investigation into `ClosureLiteSimple` and `PanelAtomizerLite`

The `get_embedding_for_image` function in `search_utils.py` uses the `model.atom` method (which is an instance of `PanelAtomizerLite`) to generate embeddings.

**Original `PanelAtomizerLite.forward` structure:**
```python
class PanelAtomizerLite(nn.Module):
    # ...
    def forward(self, images, input_ids, attention_mask, comp_feats):
        V = self.vision(images)
        T = self.text(input_ids, attention_mask)
        C = self.comp(comp_feats)
        P = self.fuse(V, T, C)  # (B,D)
        return P
```

**Analysis:**
1.  **Multi-modal Inputs:** The `PanelAtomizerLite` is designed as a multi-modal encoder, expecting `images`, `input_ids` (text), and `comp_feats` (compositional features).
2.  **Dummy Inputs:** In `get_embedding_for_image`, when only an image was provided, `input_ids`, `attention_mask`, and `comp_feats` were being supplied as `torch.zeros` or `torch.ones` tensors.
3.  **Fusion Dominance:** The `GatedFusion` module (`self.fuse`) combines the outputs of the vision (`V`), text (`T`), and composition (`C`) encoders. If `T` and `C` were constant (due to constant zero/dummy inputs), the `GatedFusion` could have learned to assign significant weight to these constant components, effectively "drowning out" the unique features from the image (`V`). This resulted in a final fused embedding (`P`) that was largely independent of the actual image input.

**Attempted Fixes & Observations:**
*   **Passing `None` for dummy inputs:** An attempt was made to pass `None` for `input_ids`, `attention_mask`, `comp_feats`, and `panel_mask`. This resulted in a `RuntimeError: You have to specify either input_ids or inputs_embeds`, indicating the model explicitly requires these inputs.
*   **Adjusting dummy tensor shapes:** The shapes of the `torch.zeros` dummy inputs were adjusted to match expected single-item batch dimensions. While this resolved the `RuntimeError`, the `E_page` sample and similarity results remained identical across different image uploads, confirming the fusion dominance issue.
*   **Direct Vision Encoder Access (Attempt 1):** Modified `get_embedding_for_image` to directly call `model.atom.vision(images)`. This *did* produce discriminative embeddings from `model.atom.vision`. However, this bypassed the `model.han.panels_to_page` aggregation, leading to a mismatch in embedding space with the Zarr dataset's `page_embeddings` (which are aggregated). The similarity scores remained low, but the `E_page` sample *was* discriminative.
*   **Replicating Zarr Generation Process (Attempt 2):** Modified `get_embedding_for_image` to fully replicate the Zarr page embedding generation process: `model.atom` (with dummy text/comp features) -> `model.han.panels_to_page`. This *reverted* the behavior to the original problem: the `E_page` sample became constant again, and the similarity scores were low and identical across different image uploads.

## Current Problem: Image Input Not Influencing Final Embedding (Again)

Despite attempts to ensure the image input is processed, the `E_page` embedding generated by `get_embedding_for_image` (after passing through `model.atom` and `model.han.panels_to_page`) is still constant across different image uploads. This indicates that the image input is not effectively influencing the final page-level embedding when dummy text and compositional features are provided.

**Hypothesis:** The `GatedFusion` within `PanelAtomizerLite` is still giving too much weight to the constant (dummy) text and compositional features, effectively overriding the discriminative image features, even when the `ViTEncoder` (`model.atom.vision`) is producing unique outputs.

## Module Loading/Caching Issue

During debugging, `print` statements added to `closure_lite_simple_framework.py` (specifically in `StoryHANLiteSimple.panels_to_page` and the `__init__` methods) were not appearing in the console, even though the functions were being executed. This strongly suggests a Python module loading or caching issue, where the server is loading an older or different version of `closure_lite_simple_framework.py` than the one being modified.

## Latest Fix Attempt: Bypassing `PanelAtomizerLite`'s Fusion for Image-Only Embeddings

**Problem Identified:** The `P_flat` output from `model.atom` was constant, even when `V_flat` (from `model.atom.vision`) was discriminative. This confirmed that the `GatedFusion` within `PanelAtomizerLite` was overriding the image features with constant values from the dummy text and compositional inputs.

**Solution:** Modify `get_embedding_for_image` to directly use the discriminative `V_flat` (output of `model.atom.vision`) as `P_flat`, thereby bypassing the problematic fusion step within `model.atom` for image-only queries. This `P_flat` is then passed through `model.han.panels_to_page` for page-level aggregation.

**Change in `search_utils.py` (within `get_embedding_for_image`):**
```python
    # ...
    with torch.no_grad():
        # Flatten images for vision encoder (B*N, C, H, W)
        images_flat = batch['images'].flatten(0,1)

        # 1. Get pure vision embedding for the panel
        V_flat = model.atom.vision(images_flat) # (B*N, D)
        print(f"DEBUG: V_flat sample (first 5 elements): {V_flat.cpu().numpy()[0, :5]}")

        # Use V_flat directly as P_flat, bypassing text, comp, and fusion for image-only queries
        P_flat = V_flat
        print(f"DEBUG: P_flat sample (first 5 elements): {P_flat.cpu().numpy()[0, :5]}")

        P = P_flat.view(B, N, -1) # Reshape to (B, N, D) to represent a single panel as a page

        # 2. Pass through page-level understanding (even for a single panel/page)
        E_page, _ = model.han.panels_to_page(P, batch['panel_mask'])
        print(f"DEBUG: E_page sample (first 5 elements): {E_page.cpu().numpy()[0, :5]}")

    return E_page.cpu().numpy()
```

**Expected Outcome:**
After this change, `V_flat`, `P_flat`, and `E_page` samples should all be discriminative and different for different image inputs. The similarity search should then yield relevant results, with a higher maximum similarity for an uploaded image that is identical to one in the dataset.

## Current State of Debugging (After Latest Change)

**Observation:** The `V_flat` sample is discriminative, but `P_flat` and `E_page` samples are *still constant* across different image uploads. This indicates that the image input is still not effectively influencing the final page-level embedding, even after directly using `V_flat` as `P_flat` and passing it through `model.han.panels_to_page`.

**Hypothesis:** The `model.han.panels_to_page` function itself might be producing a constant output when given a single panel embedding, or there's a deeper issue with how the model is configured for single-panel inputs.

**Next Step:** Debug `model.han.panels_to_page` by inspecting its inputs and outputs. However, due to module loading/caching issues, direct prints within `closure_lite_simple_framework.py` are unreliable.

## Debugging `model.atom` inputs and outputs (Current Focus)

**Strategy:** Inspect the inputs and output of `model.atom` within `search_utils.py` to confirm if `model.atom` is indeed producing a constant `P_flat` even when `images_flat` is discriminative.

**Change in `search_utils.py` (within `get_embedding_for_image`):**
```python
    with torch.no_grad():
        # Flatten images for vision encoder (B*N, C, H, W)
        images_flat = batch['images'].flatten(0,1)
        
        # Recreate dummy inputs, as they are needed for model.atom
        input_ids_flat = torch.zeros((B*N, 128), dtype=torch.long).to(device)
        attention_mask_flat = torch.zeros((B*N, 128), dtype=torch.long).to(device)
        comp_feats_flat = torch.zeros((B*N, 7), dtype=torch.float32).to(device)

        print(f"DEBUG: images_flat sample (first 5 elements): {images_flat.cpu().numpy()[0, 0, :5]}")
        print(f"DEBUG: input_ids_flat sample (first 5 elements): {input_ids_flat.cpu().numpy()[0, :5]}")
        print(f"DEBUG: comp_feats_flat sample (first 5 elements): {comp_feats_flat.cpu().numpy()[0, :5]}")

        # 1. Panel Analysis (raw embeddings) - using full model.atom
        P_flat = model.atom(images_flat, input_ids_flat, attention_mask_flat, comp_feats_flat)
        print(f"DEBUG: P_flat sample (first 5 elements): {P_flat.cpu().numpy()[0, :5]}")

        P = P_flat.view(B, N, -1) # Reshape to (B, N, D) to represent a single panel as a page

        # 2. Pass through page-level understanding (even for a single panel/page)
        E_page, _ = model.han.panels_to_page(P, batch['panel_mask'])

    return E_page.cpu().numpy()
```

**Expected Outcome:**
This will confirm if `model.atom` is indeed producing a constant `P_flat` even when `images_flat` is discriminative. If `P_flat` is constant, it points to the `GatedFusion` being the culprit. If `P_flat` is discriminative, then the problem lies in `model.han.panels_to_page`.

## Image Tensor Transformations in `search_utils.py`

The `get_embedding_for_image` function in `search_utils.py` performs several transformations on the input image before it's fed into the model. These transformations are crucial for preparing the image data in a format expected by the pre-trained vision model.

The transformation pipeline is defined as:
```python
    tf = T.Compose([
        T.Resize((224, 224)),
        T.ToTensor(),
    ])
```

Here's a breakdown of each step:

1.  **`T.Resize((224, 224))`**:
    *   **Purpose:** This transformation resizes the input image to a fixed dimension of 224 pixels by 224 pixels. Many pre-trained convolutional neural networks (CNNs), like the Vision Transformer (ViT) used in this project, are trained on images of a specific size (e.g., 224x224 or 256x256). Resizing ensures that the input image matches the expected dimensions of the model.
    *   **Effect on the example:** The raw image with dimensions `(1988, 3057)` is scaled down to `(224, 224)`.

2.  **`T.ToTensor()`**:
    *   **Purpose:** This transformation converts the image from a PIL (Pillow) Image object or a NumPy `ndarray` into a PyTorch `FloatTensor`. It also performs two critical operations:
        *   **Pixel Value Scaling:** It scales the pixel intensity values from the typical `[0, 255]` integer range (common for image files) to a `[0.0, 1.0]` floating-point range. This normalization is standard practice for neural networks, as it helps with training stability and performance.
        *   **Dimension Reordering:** It rearranges the dimensions of the image tensor. PIL Images and NumPy arrays typically store images in a `(Height, Width, Channels)` format (e.g., `(H, W, 3)` for an RGB image). `T.ToTensor()` converts this to `(Channels, Height, Width)` (e.g., `(3, H, W)`), which is the standard format expected by PyTorch convolutional layers.
    *   **Effect on the example:** After resizing, the image's pixel values are converted to floats between 0.0 and 1.0, and its shape becomes `(3, 224, 224)`.

**Batch and Panel Dimensions (`unsqueeze` operations):**

After the `T.Compose` transformations, the resulting tensor has a shape of `(C, H, W)`, which is `(3, 224, 224)`. However, the model expects a batch of images, and in this specific `CoMix` framework, it also expects a dimension for "panels per page". This is handled by two `unsqueeze(0)` operations:

```python
batch = {
    'images': tf(img).to(device).unsqueeze(0).unsqueeze(0), # (B, N, C, H, W)
    # ...
}
```

*   **First `unsqueeze(0)`:** Adds a batch dimension at the beginning. The shape changes from `(C, H, W)` to `(1, C, H, W)`. Here, `1` represents a batch size of one image.
*   **Second `unsqueeze(0)`:** Adds another dimension, representing the "number of panels per page" (`N`). In this case, for a single image query, `N` is `1`. The shape becomes `(1, 1, C, H, W)`.

Therefore, the final shape of the `images` tensor in the `batch` dictionary is `torch.Size([1, 1, 3, 224, 224])`, which corresponds to `(Batch Size, Number of Panels, Channels, Height, Width)`.

**Summary of Transformed Data:**

The debug output for the transformed image tensor shows:
*   `torch.Size([1, 1, 3, 224, 224])`: Confirms the expected dimensions after resizing and unsqueezing.
*   `mean: 0.4310, std: 0.3069`: These values are typical for image data normalized to the `[0, 1]` range, indicating that the `T.ToTensor()` scaling worked correctly.
*   `sample (first 5 elements): tensor([0.6745, 0.6745, 0.6784, 0.7255, 0.7882])`: These are floating-point values between 0 and 1, corresponding to the scaled pixel intensities. The slight variation from the raw pixel values (e.g., `172/255 = 0.6745`) and the presence of different values confirms that the image data is being processed and is not uniform. The similarity in the first few values is consistent with the raw image showing a large area of similar color (blue sky).
*   `sample (last 5 elements): tensor([0.0353, 0.0353, 0.0353, 0.0353, 0.0353])`: Similarly, these values correspond to the scaled pixel intensities of the darker border area, again showing consistency with the raw image.

In conclusion, the image tensor transformations are correctly applied, resizing the image, normalizing pixel values, reordering dimensions, and adding batch/panel dimensions as required by the model architecture. The debug output confirms that the transformed image data is diverse and correctly prepared for the model.

## Further Debugging: Mismatch in Embedding Spaces

**Observation:** After ensuring `V_flat` (pure vision embedding) was discriminative, and attempting to align the query input structure with the Zarr dataset's multi-panel page embeddings, the search results for both "page" and "panel" modes remained poor. Specifically, "panel" search returned "mostly text pages" with "wide rectangle highlighting empty space," indicating a lack of semantic relevance.

**Root Cause Identified:**
A deeper analysis of `generate_embeddings_zarr.py` and `closure_lite_framework.py` revealed that the `panel_embeddings` stored in the Zarr dataset are **fused multi-modal embeddings**. They are the direct output of `model.atom(images, input_ids, attention_mask, comp_feats)`, meaning they incorporate contributions from vision, text, and compositional features.

Our previous approach in `search_utils.py` (where `P_flat` was directly assigned `V_flat`) resulted in a *pure vision embedding* for the query. This created a fundamental mismatch: we were comparing a pure vision embedding (from the query) against fused multi-modal embeddings (in the Zarr dataset). These two types of embeddings reside in different semantic spaces, leading to poor search performance.

**Solution Implemented (Re-aligning Query Embedding Generation):**
To resolve this, the `get_embedding_for_image` function in `search_utils.py` was modified to generate a *fused multi-modal embedding* for the query image, mirroring the process used during Zarr dataset generation.

The specific changes are:
1.  **Reverted `P_flat = V_flat`:** The direct assignment was removed, and `model.atom` is now called with all its inputs (`images_flat`, `input_ids_flat`, `attention_mask_flat`, `comp_feats_flat`).
2.  **Meaningful Dummy `comp_feats_flat`:** For a single image query, `comp_feats_flat` are now constructed to represent a full-page panel. Assuming a 224x224 input image, these features are `[1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5]` (aspect ratio, size ratio, zero character counts, zero shot mean/max, and center coordinates). This provides a more realistic spatial context for the single query image.
3.  **Generic Dummy `input_ids_flat` and `attention_mask_flat`:** These are generated by tokenizing an empty string using `AutoTokenizer.from_pretrained('roberta-base')`. This provides the necessary text inputs to `model.atom` without introducing specific text content.

**Expected Outcome:**
With these changes, `P_flat` (the panel embedding returned by `get_embedding_for_image`) is now a fused multi-modal embedding, directly comparable to the `panel_embeddings` in the Zarr dataset. Similarly, `E_page` (the page embedding) is derived from this fused `P_flat` in a multi-panel context. This should lead to significantly more relevant and discriminative search results for both "panel" and "page" search modes when querying with an image.

## Conclusion: The Multi-Modal Mismatch for Single-Modality Queries

After extensive debugging, including:
- Verifying image loading and transformation.
- Ensuring discriminative vision embeddings (`V_flat`).
- Attempting to align query input structure with Zarr dataset generation (simulated multi-panel context).
- Experimenting with both direct vision embedding (`V_flat`) and fused multi-modal embedding (`P_flat`) for queries.

The consistent observation is that single-modality (image-only) queries against the `ClosureLiteSimple` model's multi-modal embedding space yield poor and non-discriminative search results.

**Root Cause:**
The `ClosureLiteSimple` model is fundamentally a **multi-modal model**. It was trained to create embeddings by fusing information from:
1.  **Vision:** Image crops of panels.
2.  **Text:** Extracted text content from panels.
3.  **Compositional Features:** Spatial layout and characteristics of panels.

The `panel_embeddings` and `page_embeddings` stored in the Zarr dataset are **fused multi-modal embeddings**, incorporating all these modalities.

When a query is performed with only a single modality (e.g., an image), a fundamental mismatch arises:
-   **Pure Vision Query vs. Fused Target:** If we force the query embedding to be purely vision-based (by bypassing `GatedFusion`), it exists in a different semantic space than the multi-modal target embeddings in the Zarr dataset. Direct comparison is ineffective.
-   **Dummy Inputs and Fusion Bias:** If we provide dummy (zero) inputs for the missing modalities (text and compositional features) to `model.atom`'s `GatedFusion`, the fusion mechanism, having been trained on rich multi-modal data, tends to either suppress the real input or get biased by the dummy inputs, leading to a non-discriminative fused output.

**Implications for Single-Modality Queries:**
The `ClosureLiteSimple` model, in its current form and training, is not well-suited for effective single-modality (image-only or text-only) queries against its multi-modal embedding space. Its strength lies in understanding the *relationships and combined context* of all modalities.

**Path Forward for Effective Search:**
For truly effective and semantically relevant search results using this framework, the query itself needs to be **multi-modal**. This means that for an uploaded image, a comprehensive pipeline would be required to generate a query embedding that is comparable to the Zarr dataset's embeddings:
1.  **Panel Detection:** Run a panel detection model on the query image to identify individual panels.
2.  **VLM Analysis:** Extract text and other relevant features from each detected panel using a Vision-Language Model.
3.  **Compositional Feature Calculation:** Compute spatial features for each panel.
4.  **Fused Embedding Generation:** Feed these multi-modal inputs (image crops, extracted text, compositional features) into the `ClosureLiteSimple` model to generate a fused multi-modal `P_flat` (panel embedding) and `E_page` (page embedding).

Without such a multi-modal query generation pipeline, single-modality queries will continue to struggle to find meaningful matches in the model's multi-modal embedding space. The model is likely "good" for its intended multi-modal purpose, but its direct application to single-modality queries is limited by its design.

## Future Work: Improving Text Handling

Our analysis has revealed that the text modality, as currently used, often contains descriptive text generated by the VLM rather than literal text transcribed from the page. This has significant implications for the model's training and its ability to understand the page's content semantically.

To address this, the following future work should be considered:

1.  **Differentiate Text Types:** The data processing pipeline should be updated to differentiate between different types of text. Instead of a single `narration` field, we should aim for a more structured format that separates:
    *   **Literal Text:** Text that is visibly present on the page (e.g., dialogue, captions, sound effects).
    *   **Descriptive Text:** Scene descriptions, character actions, or other contextual information generated by the VLM.

2.  **Use OCR as a Baseline:** To reliably get the literal text from the page, we should incorporate a dedicated OCR tool into our data pipeline. The output of the OCR can serve as a ground truth for the literal text.

3.  **Refine VLM Prompting:** The prompts used for the VLM should be refined to explicitly ask for this separation. The VLM could be prompted to return a JSON object with distinct fields for `literal_text` and `scene_description`.

4.  **Update Model Architecture:** The model itself could be updated to handle these different text types as separate features. This would allow it to learn the difference between what is written on the page and what is being described, potentially leading to a more nuanced and accurate understanding of the comic page.

By implementing these changes, we can improve the quality of our text data and, consequently, the semantic understanding of our models.

## Cluster Analysis Insights (from `combo_analysis_report.csv`)

Analysis of the combined report, which includes UMAP/clustering data, compositional stats, and detailed text analysis, has revealed several distinct page archetypes learned by the model. The clustering appears to be heavily influenced by compositional features (like panel count and page orientation) and high-level text features (like the presence and type of text), rather than semantic content.

### Key Cluster Archetypes Identified:

-   **Splash/Cover/Back Page Clusters (e.g., Clusters 1, 7):** These clusters are characterized by a very low average panel count (typically 1-2 panels). They are often dominated by "narration_only" text, where the VLM provides a description of the single large image.
-   **Text-Heavy Content Cluster (e.g., Cluster 6):** This cluster has a low panel count but an extremely high average narration length. Investigation suggests this cluster has erroneously captured non-comic book content, such as pages from programming books that were accidentally included in the dataset. This highlights a need for better data filtering.
-   **"Typical" Multi-Panel Page Clusters (e.g., Clusters 0, 2, 3, 4, 8, 9):** These clusters represent the bulk of the dataset, with a higher average panel count (5-7) and a mix of dialogue and narration.

### Hypotheses and Future Investigation:

-   **Landscape Page Orientation:** A strong hypothesis is that landscape-oriented pages are being grouped together. Since landscape orientation is less common in the dataset, it acts as a strong compositional feature.
-   **R-CNN Performance on Landscape Pages:** It is also hypothesized that the R-CNN model may be less effective at detecting panels on landscape pages, leading to an incorrect low panel count for these pages and causing them to be clustered with true single-panel pages.
-   **"Narration Only" as a Feature:** The model appears to be using the *type* of text as a feature. Pages with only VLM-generated narration are being grouped together, regardless of the semantic content of that narration.

These findings suggest that future work should focus on improving data cleaning (e.g., filtering out non-comic content) and potentially developing features that can explicitly account for page orientation.

### Deeper Dive: The "Prose-Page" Archetype and Narration Types

Further investigation into the clusters has revealed more nuanced patterns:

-   **Cluster 7 (Splash/Cover/Marker Pages):** A closer look at Cluster 7 confirms that it contains single-panel splash pages, covers, and chapter markers. These pages share a common structure of being single, large images, often with a small amount of text.

-   **Data Cleaning Issues:** The analysis also uncovered new data quality issues, such as the presence of watermarks from sources like DriveThruRPG on some pages. These watermarks introduce noise into the visual data.

-   **The "Prose-Page" Archetype:** Our investigation into why semantically different pages (e.g., an "about the creators" page and an editorial) are considered highly similar has led to the identification of a "prose-page" archetype. The model has learned a strong representation for pages that consist of a single large image area combined with a large block of prose text. This category includes:
    *   Editorials
    *   "About the creators" pages
    *   Letters pages (hypothesis pending verification)
    *   Other text-heavy supplementary content.

-   **"Extant" vs. "Descriptive" Narration:** This led to the crucial distinction between two types of text being conflated in the `narration` field:
    *   **Extant Text Narration:** Literal prose text that is actually present on the page (e.g., an editorial).
    *   **Descriptive Narration:** A scene description generated by the VLM for a page that may have little or no actual text.

    *   **DON'T** commit large data files, datasets, or model checkpoints to the git repository.

## Analysis of Vision-Only Ablation Clusters (UMAP and Statistical Data)

Following the initial debugging of the multi-modal model's single-modality query issues, a deeper analysis was conducted on the vision-only ablation clusters generated from `combined_analysis_report_finalvision.csv`. The goal was to understand what visual features the `ClosureLiteSimple` model's vision encoder prioritizes when operating solely on pixel data.

### General Findings:
*   **Ribbon Structure in UMAP:** The UMAP projection consistently shows "ribbon-like" structures for clusters. This indicates that the model has learned continuous progressions in certain visual features, rather than discrete, isolated groups.
*   **Structural/Visual Bias:** The vision-only model primarily clusters pages based on structural and aesthetic visual features (e.g., panel count, layout, darkness, contrast, visual sparsity), rather than semantic content.
*   **Non-Discriminative Similarity:** As observed in the web interface, similarity scores for vision-only queries remain high and non-discriminative for semantic search, reinforcing the multi-modal mismatch problem.

### Refined Cluster Archetypes and Observations:

#### Cluster 4: The "Dominant Panel" Archetype (Refined)
*   **Initial Hypothesis:** Pure "Splash/Cover Page" cluster due to lower panel count (mean 4.3) and high average panel area.
*   **Refinement:** An average panel count of 4.3 is too high for a pure splash page cluster. This cluster likely represents pages characterized by a **"dominant panel" layout**. These are pages featuring one very large, impactful panel alongside a few smaller, supplementary panels. This explains both the lower-than-average panel count and the high average panel area.

#### Cluster 8: The "Prose Page" / "Visually Sparse/Atmospheric" Archetype (Detailed Analysis)
*   **Statistical Profile:** Characterized by a very low average panel count (mean 1.6), extremely high average narration character count (mean 506), and almost zero dialogue (mean 7.9).
*   **Core Visual Archetype ("Prose Page"):** Pages like `00132.jpg` (Hellboy Volume 6) are classic examples. These are visually dominated by large blocks of text, often against a plain background. The vision model recognizes the visual pattern of dense text blocks, even without "reading" the text.
*   **Visual Sparsity and Atmosphere:** The cluster also includes pages with higher panel counts (e.g., 2-panel `image-227.jpg`, 3-panel `image-007.jpg`, 4-panel `image-011.jpg`, 7-panel `0009.jpeg`, 9-panel `00015.jpg`). These pages share a common visual aesthetic:
    *   **Dominant Darkness/High Contrast:** Large areas of black, strong contrasts between light and shadow.
    *   **Minimalist/Sparse Visuals:** Limited visual information, often stylized or repetitive elements.
    *   **Atmospheric Tone:** Creates a specific mood (e.g., suspense, mystery, somberness).
*   **Outliers and Subgroups:**
    *   **High Panel Count Outliers:** Pages like the 9-panel `00015.jpg` and 7-panel `0009.jpeg` are outliers in terms of panel count but fit the "dark and atmospheric" visual theme.
    *   **"All White" Subgroup:** A disconnected small group within Cluster 8 (e.g., "prodigy slaves of mars page 2") is characterized by being "all white." This highlights that the overarching theme for Cluster 8 is **visual sparsity** or **minimalism**, encompassing both dark and light extremes that lack the visual complexity of typical comic pages.
*   **Clarification on VLM Narration and Vision Embeddings:** It is crucial to reiterate that the vision-only embeddings are generated *solely* from pixel data. The `narration_char_count` from the VLM analysis was **NOT** used to create these embeddings or clusters. The observed correlation between Cluster 8's visual characteristics and high narration counts is because the vision model successfully identifies the *visual archetype* of pages (e.g., prose pages, atmospheric pages) that, in a separate VLM analysis, would naturally elicit extensive narration. The statistical correlation is a symptom of the visual clustering, not its cause.

#### UMAP Axis Interpretation (Hypothesis):
*   The `UMAP_dim1` (horizontal axis) appears to correlate strongly with **panel shape and layout regularity**. Moving from left to right, pages may transition from more irregular/elongated panels to more uniform/square-like panels.

### Next Steps for Investigation:
Based on statistical anomalies and potential contrasts, the following clusters are recommended for further visual investigation:

*   **Cluster 5: The "Extreme Aspect Ratio" Cluster:** Characterized by an `avg_panel_aspect_ratio` of 4.89 (a significant outlier) and the highest average narration count. This cluster likely contains pages with unusual layouts (e.g., extremely wide/tall panels, landscape orientation) or potentially non-comic content.
*   **Cluster 6: The "High-Density Grid" Cluster:** Characterized by the highest average `panel_mask_sum` (6.57). This cluster should represent the most dense, grid-like pages, providing a strong contrast to the sparse pages of Cluster 8.

