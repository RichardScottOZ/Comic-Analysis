# Multi-Dataset Search Interface

## Overview

The search interface now supports querying multiple embedding datasets with different modality configurations. This allows you to compare how different embedding approaches (vision-only, text-only, composition-only, or fully-fused multimodal) perform for the same query.

## Available Datasets

The interface can load and query from four different embedding datasets:

### 1. Fused (Default)
- **Path**: `E:\calibre3\combined_embeddings.zarr`
- **Description**: Full multimodal embeddings with all modalities fused
- **Use Case**: Best overall performance, leverages vision, text, and compositional information together
- **Generated by**: `generate_embeddings_zarr_claude.py`

### 2. Vision Only
- **Path**: `E:\calibre3_vision\combined_embeddings.zarr`
- **Description**: Pure vision-based embeddings (images only)
- **Use Case**: Study the effectiveness of visual features alone, useful for finding visually similar pages even if text differs
- **Generated by**: `generate_vision_embeddings_zarr_claude.py`

### 3. Text Only
- **Path**: `E:\calibre3_text\combined_embeddings.zarr`
- **Description**: Pure text-based embeddings (text only)
- **Use Case**: Analyze text-based similarity, find pages with similar dialogue/narration regardless of visual appearance
- **Generated by**: `generate_text_embeddings_zarr_claude.py`

### 4. Composition Only
- **Path**: `E:\calibre3_comp\combined_embeddings.zarr`
- **Description**: Pure compositional embeddings (layout only)
- **Use Case**: Identify pages with similar panel layouts and reading order, independent of content
- **Generated by**: `generate_comp_embeddings_zarr_claude.py`

## How to Use

### Starting the Server

```bash
python search_server.py
```

The server will:
1. Attempt to load all four datasets
2. Skip any datasets that don't exist yet (with a warning message)
3. Use 'fused' as the default if available, otherwise the first available dataset
4. Start on port 5001

### Web Interface

1. **Query Type Selection**: Choose between Image Search, Text Search, or Page ID Search (unchanged)

2. **Dataset Selection**: NEW dropdown menu allows you to select which embedding dataset to query
   - The dropdown shows only datasets that were successfully loaded
   - A description appears below the dropdown explaining the selected dataset
   - Selection persists across searches in the same session

3. **Search Modes**: Panel or Page search options (unchanged)

4. **Results Display**: 
   - A banner at the top of results shows which dataset was used
   - Results are ranked by similarity score as before

### Generating the Ablation Datasets

Before all four datasets will be available, you need to generate the three ablation datasets:

```bash
# Generate vision-only embeddings
python benchmarks/detections/openrouter/generate_vision_embeddings_zarr_claude.py \
    --checkpoint "C:\Users\Richard\OneDrive\GIT\CoMix\closure_lite_output\calibre_perfect_simple_denoise_context\best_checkpoint.pth" \
    --amazon_json_list .\perfect_match_training\calibre_dataspec_final_perfect_list.txt \
    --amazon_image_root "E:\CalibreComics_extracted" \
    --output_dir "E:\calibre3_vision" \
    --batch_size 16 \
    --device auto

# Generate text-only embeddings  
python benchmarks/detections/openrouter/generate_text_embeddings_zarr_claude.py \
    --checkpoint "C:\Users\Richard\OneDrive\GIT\CoMix\closure_lite_output\calibre_perfect_simple_denoise_context\best_checkpoint.pth" \
    --amazon_json_list .\perfect_match_training\calibre_dataspec_final_perfect_list.txt \
    --amazon_image_root "E:\CalibreComics_extracted" \
    --output_dir "E:\calibre3_text" \
    --batch_size 16 \
    --device auto

# Generate composition-only embeddings
python benchmarks/detections/openrouter/generate_comp_embeddings_zarr_claude.py \
    --checkpoint "C:\Users\Richard\OneDrive\GIT\CoMix\closure_lite_output\calibre_perfect_simple_denoise_context\best_checkpoint.pth" \
    --amazon_json_list .\perfect_match_training\calibre_dataspec_final_perfect_list.txt \
    --amazon_image_root "E:\CalibreComics_extracted" \
    --output_dir "E:\calibre3_comp" \
    --batch_size 16 \
    --device auto
```

## Research Applications

### Modality Ablation Studies

Compare the same query across different datasets to understand:

- **Which modality matters most** for different types of queries
- **How much each modality contributes** to the fused representation
- **When visual vs. textual similarity** is more important
- **Layout patterns** independent of content

### Example Experiments

1. **Visual Style Study**: Query with an image, compare results across Vision-Only vs. Fused to see how much text biases visual similarity

2. **Content vs. Form**: Use Page ID search to select a page, then query as embedding across all four datasets to see which aspects of similarity each captures

3. **Layout Analysis**: Compare Composition-Only results to understand pure structural similarity

4. **Text Semantic Search**: Compare Text-Only semantic search vs. Fused to see if visual context improves text understanding

## Technical Details

### Server Architecture

- **Single Model Instance**: One model checkpoint is loaded and shared across all searches
- **Multiple Dataset Instances**: Each Zarr dataset is loaded once at startup and kept in memory
- **Dynamic Dataset Selection**: Search functions receive the selected dataset as a parameter

### Dataset Configurations

Dataset configurations are defined in `search_server.py`:

```python
EMBEDDING_CONFIGS = {
    'fused': {
        'name': 'Fused (Vision + Text + Composition)',
        'zarr_path': "E:\\calibre3\\combined_embeddings.zarr",
        'description': 'Full multimodal embeddings with all modalities fused'
    },
    # ... more configs
}
```

To add a new dataset, simply add an entry to this dictionary and restart the server.

### Modified Files

- `search_server.py`: Multi-dataset loading, dataset selection parameter handling
- `search_utils.py`: Functions updated to accept dataset parameter (backward compatible)
- `templates/index.html`: Dropdown selector UI, dataset info display

## Performance Considerations

- **Memory**: Each dataset is ~300-400MB in memory, so loading all four uses ~1.2-1.6GB
- **Query Speed**: No performance difference between datasets - all use the same search algorithm
- **Startup Time**: Loading four datasets adds ~10-20 seconds to server initialization

## Troubleshooting

**Issue**: Dataset not appearing in dropdown
- **Solution**: Check if the Zarr file exists at the configured path. Server logs will show which datasets loaded successfully.

**Issue**: Results look identical across datasets
- **Solution**: Verify that the ablation datasets were generated correctly using the modality-specific scripts, not the standard generate_embeddings_zarr.py

**Issue**: Search returns error after switching datasets
- **Solution**: Some datasets might have slightly different manifest paths or missing pages. Check server logs for details.

## Future Enhancements

Possible extensions to this interface:

1. **Side-by-side comparison**: Display results from multiple datasets simultaneously
2. **Similarity metrics**: Show per-dataset similarity scores for the same pages
3. **Dataset statistics**: Display embedding statistics and coverage information
4. **Dynamic weighting**: Allow user-specified weights for modality fusion
5. **Custom dataset upload**: Support for user-generated embedding datasets
