

# **Beyond the Bounding Box: A Multi-Modal, Sequence-Aware Framework for Character Identification in Comic Book Narratives**

## **Part I: The Fragility of Standard Detectors in the Comics Domain**

The computational analysis of comic book narratives, a medium that uniquely fuses static imagery with rich textual and sequential elements, has long been a subject of academic inquiry.1 While early efforts focused on foundational tasks such as panel and speech balloon segmentation, the ultimate goal of understanding narrative requires a robust solution to a more fundamental problem: character identification. This task, which involves detecting characters within panels and consistently recognizing them across an entire story, is the bedrock upon which all higher-level narrative analysis—from dialogue attribution to plot summarization—is built. However, the application of standard, state-of-the-art object detection models to this domain has yielded consistently mediocre results, exposing a fundamental disconnect between the assumptions of these models and the realities of the comic book medium.

This section establishes the foundational argument that this performance gap is not an incremental challenge to be solved with better backbones or more training data, but rather a paradigm mismatch. Standard object detectors are architecturally ill-suited for a task that is less about recognizing visually consistent objects and more about tracking persistent narrative entities through a highly variable and unreliable visual stream. By critically analyzing recent benchmark results and developing a specific taxonomy of failure modes unique to the comics domain, it becomes clear that a new approach is required—one that re-frames character identification as a multi-modal, sequence-aware tracking problem.

### **1.1. A Critical Analysis of Current Benchmarks: The Performance Ceiling**

The persistent challenges in the computational analysis of comics have historically been attributed to a fragmented and difficult research landscape. Key issues include the prevalence of small, private datasets, inconsistent annotation standards between different comic styles (e.g., Japanese manga, American comics, and Franco-Belgian *bandes dessinées*), and a lack of publicly available model weights, all of which severely hamper reproducibility and comparative analysis.1 In response to these systemic problems, recent work has focused on creating unified benchmarks to provide a clear and replicable evaluation setting.

A pivotal study in this area is presented in arXiv:2407.03540v1, which introduces the Comics Datasets Framework (CDF).1 This framework standardizes annotations from major datasets like Manga109, eBDtheque, and DCM, and introduces a new annotated dataset, Comics100, to balance the overrepresentation of manga styles.1 Using this unified framework, the authors benchmarked a variety of standard object detection architectures, including established two-stage and one-stage detectors like Faster R-CNN and YOLOv8, respectively, as well as more recent zero-shot models like Grounding DINO.1

While the paper's primary contribution is the creation of the benchmark itself, the performance of these state-of-the-art models is revealing. The reported metrics for character and face detection, while representing a standardized baseline, are notably modest compared to the performance of the same models on real-world photographic datasets like COCO. For instance, prior state-of-the-art results on various comic datasets for character detection show mean Average Precision (mAP) scores around 41.7, with precision and recall figures often showing a significant trade-off (e.g., 75.4 Precision but only 9.8 Recall).1 These figures, while difficult to compare directly due to the aforementioned inconsistencies, paint a picture of a problem that is far from solved.

The mediocre results of these powerful, general-purpose object detectors should not be interpreted as a simple performance gap that can be closed with more data or larger models. Instead, they represent a *performance ceiling* that is indicative of a fundamental architectural and conceptual mismatch. These models are designed to learn and recognize categories of objects that exhibit a reasonable degree of visual consistency. However, as will be explored, the "category" of a specific comic book character is subject to extreme variations in appearance that violate the core assumptions of these architectures. Consequently, simply applying more powerful versions of the same models is likely to yield diminishing returns. The problem is not that the models are not powerful enough, but that they are being asked the wrong question—to perform static object detection on what is inherently a dynamic, context-dependent tracking task.

### **1.2. A Taxonomy of Failure Modes in the Comics Domain**

To understand why standard detectors hit this performance ceiling, it is necessary to move beyond aggregate metrics like mAP and perform a qualitative failure analysis. By synthesizing general frameworks for categorizing object detection errors, such as TIDE (Toolkit for Identifying Detection Errors), with the unique visual language of comics, a more precise diagnosis emerges.5 TIDE categorizes errors into six types: classification error (Cls), localization error (Loc), both classification and localization error (Cls+Loc), duplicate detection error, background error, and missed ground-truth error.5 While these are all present in the comics domain, they are often caused by a set of underlying factors specific to the medium. This allows for the creation of a new, domain-specific taxonomy of failure modes.

**Table 1.2: Taxonomy of Object Detector Failure Modes in the Comics Domain**

| Failure Category | Sub-Category | Description | Root Cause | Example (Conceptual) |
| :---- | :---- | :---- | :---- | :---- |
| **Standard Errors** | Classification (Cls) | An object is correctly localized but assigned the wrong class. | Inter-class visual similarity. | A detector confuses one superhero in a cape for another. |
|  | Localization (Loc) | An object is correctly classified but its bounding box has low IoU with the ground truth. | Ambiguous object boundaries, unusual aspect ratios. | A character's flowing cape or dynamic pose leads to an inaccurate bounding box. |
|  | Missed Ground Truth | An object is present but the detector fails to produce any corresponding detection. | Low model confidence, out-of-distribution features. | A character drawn at a very small scale in a crowd scene is missed entirely. |
| **Domain-Specific Errors** | **Style-Shift Error** | A detector trained on one artistic style fails to generalize to another. | **Domain Shift.** The statistical distribution of visual features (line weight, color palette, shading) changes drastically between artists or eras. | A model trained on modern, clean-lined manga fails to detect characters in a heavily-inked 1940s American comic. |
|  | **Semantic Ambiguity Error** | A character's visual form changes radically due to narrative events, becoming visually unrecognizable. | **Lack of Narrative Context.** The character's identity is maintained by the story, not by visual consistency. | A shapeshifting character like Ms. Marvel transforms into a sofa; the detector correctly sees a sofa, failing to identify the character.7 |
|  | **Occlusion-by-Design Error** | Key identifying features (e.g., face, costume emblem) are obscured by narrative or layout elements. | **Structured Occlusion.** Speech bubbles, panel borders, and foreground characters are deliberately placed, systematically hiding information. | A speech bubble covers the top half of a character's head, making face recognition impossible from the visual data alone.8 |
|  | **Pose & Perspective Error** | A character is depicted in an extreme, non-humanoid, or heavily foreshortened pose. | **Out-of-Distribution Pose.** The distribution of character poses in comics (e.g., dynamic action poses) is vastly different from real-world datasets used for pre-training.9 | A character is shown leaping towards the "camera" with extreme foreshortening, distorting their proportions and confusing a model trained on human poses. |

These domain-specific failures are not edge cases; they are fundamental components of the visual language of comics.

* **Style-Shift Error:** The artistic style is a primary variable. The same character, such as Batman, can appear radically different when drawn by Frank Miller versus Jim Lee. This creates an immense domain gap not just between different comic series but potentially within a single issue if a flashback sequence is drawn by a different artist. Methods for artistic domain adaptation have shown that this is a non-trivial problem, as style affects texture, shape, and color in complex ways.11 A standard detector, trained on a finite mix of styles, will inevitably encounter an out-of-distribution style and fail.  
* **Semantic Ambiguity Error:** Comics are a narrative medium where a character's identity is a persistent abstract concept, not a fixed visual pattern. As noted in an analysis of animacy in comics, a character's form can change, as when Ms. Marvel disguises herself as a sofa.7 Without the narrative context that an entity *is* Ms. Marvel in disguise, a vision-only model has no hope of correct identification. The ground truth is defined by the narrative, but the detector only has access to the pixels.  
* **Occlusion-by-Design Error:** Unlike occlusion in real-world scenes, which is often random, occlusion in comics is a deliberate compositional choice. Speech bubbles are placed for dialogue, panel borders frame the action, and characters are layered to create a sense of depth.8 These occluders are not noise; they are part of the signal. However, they systematically hide parts of characters, making robust detection difficult. Models like CompositionalNets, which can reason from partial evidence, are better suited for this, but standard detectors that rely on holistic appearance often fail.15  
* **Pose & Perspective Error:** Superheroes and other comic characters are frequently depicted in poses that defy human anatomy and physics. The extreme perspectives and dynamic foreshortening used to convey action create visual representations that are far removed from the training data of models pre-trained on datasets of real people, like COCO.16 This leads to failures in both bounding box localization and classification. Analogous work on pose estimation for mythological figures with multiple limbs highlights the challenges that non-standard body plans present to conventional models.10

The prevalence and fundamental nature of these errors explain the performance ceiling. The visual evidence available within a single panel is often insufficient or misleading. To overcome these failures, a model must look beyond the pixels of a single image and incorporate other sources of information.

### **1.3. The Propagation of Error: A Cascade of Narrative Misunderstanding**

The failure to correctly identify characters is not a self-contained problem. In the pipeline of computational comic analysis, character identification is a foundational, "simplest" task upon which more complex, higher-level understanding depends.1 Consequently, an error at this initial stage does not merely result in a lower score on a benchmark; it triggers a cascade of failures that renders any subsequent narrative analysis invalid.

The authors of arXiv:2407.03540v1 provide a clear example of this error propagation: generating dialogue for a comic requires recognizing the page elements, linking text to characters (speaker identification), and correctly identifying multiple instances of the same character.1 The paper explicitly states, "One mistake in this process will propagate to the next stages of the pipeline making an erroneous final dialog generation".1 If the model misidentifies Character A as Character B in a panel, it will incorrectly attribute Character A's dialogue to Character B. This single error immediately corrupts the generated script and any analysis based on it.

This principle extends to virtually all downstream narrative tasks. Consider the following:

* **Character Arc Analysis:** Tracking how a character's emotional state, motivations, or relationships evolve requires consistently identifying that character across hundreds or thousands of panels. A single ID switch—where the model starts tracking Character B as if they were Character A—breaks the continuity of the arc and invalidates the entire analysis.  
* **Social Network Analysis:** Mapping the relationships between characters (e.g., who talks to whom, who appears in scenes with whom) is impossible if the identities of the nodes (the characters) are unstable.  
* **Plot Summarization:** A coherent summary of a story relies on knowing who did what. If the model cannot reliably determine "who," it cannot construct a meaningful summary of the plot.

Therefore, robust character identification and re-identification (Re-ID) is not just one task among many; it is a critical prerequisite. The high rate of missed detections and ID switches produced by standard object detectors means that they cannot provide a reliable foundation for these more advanced forms of narrative intelligence. The problem must be solved at its root by developing a system that prioritizes identity consistency above all else. This requires a fundamental shift in perspective: from treating characters as objects to be detected to treating them as narrative entities to be tracked.

This shift in perspective reveals the core issue. Standard object detectors are designed to answer the question, "What is in this image?" by categorizing visually consistent patterns. In comics, a "character" is not a visually consistent pattern but a *persistent narrative entity*. Its visual representation is merely an unreliable, context-dependent instantiation of that abstract entity. A detector looking for a consistent visual pattern for "Superman" will fail when he is drawn by a different artist, obscured by a speech bubble, or seen from an extreme angle. The model is failing because it is trying to solve a visual pattern recognition problem, when the task is actually one of narrative entity tracking. To succeed, the model must learn an abstract concept of "Superman" that is robust to these visual perturbations, which requires looking beyond the image to the textual and sequential context where the character's identity is truly grounded.

Furthermore, the historical state of the field has inadvertently reinforced this flawed problem formulation. The decades-long struggle with small, inconsistent datasets focused primarily on detection tasks (annotating panels, faces, and characters as a generic class) has created a research ecosystem where the primary tools available were detection datasets.1 Naturally, researchers applied state-of-the-art detectors to these datasets. This created a cycle of mediocre results and incremental improvements without prompting a challenge to the underlying assumption that character ID was a detection problem. The recent introduction of comprehensive, multi-task benchmarks like CoMix, which explicitly include annotations for *relationships* and *sequences*—such as character re-identification, speaker identification, and reading order—represents a crucial and necessary course correction.17 These new resources provide the data necessary to finally break the cycle and re-frame the problem correctly as one of multi-modal sequence modeling.

## **Part II: Forging Robust Identity Features from Unreliable Visuals**

Given that the visual representation of a comic book character is inherently unreliable, the first step in building a robust identification system is to develop methods for extracting visual features, or embeddings, that are as resilient as possible to the primary sources of variation: artistic style, occlusion, and pose. The goal is not to create a perfect, infallible visual signature—as Part I argued, this is impossible—but to distill a stable enough signal that it can be effectively integrated with textual and narrative context in a downstream sequence model. This section details a multi-pronged strategy for forging these robust visual embeddings by leveraging techniques from domain adaptation, occlusion-aware modeling, and pose-invariant learning.

### **2.1. Learning Style-Invariant Representations via Domain Adaptation**

The most pervasive challenge in the comics domain is the immense variance in artistic style. A character's appearance can change dramatically depending on the artist, the era of publication, the coloring techniques used, and even the narrative context (e.g., a flashback rendered in a different style). This represents a classic domain shift problem, where a model trained on a source domain (e.g., images from one artist) fails to generalize to a target domain (images from another artist).11 Unsupervised domain adaptation (UDA) offers a powerful set of techniques to learn representations that are invariant to these stylistic shifts without requiring prohibitively expensive labeled data for every new style.

A particularly promising approach is the one proposed by Thomas and Kovashka, which focuses specifically on domain differences arising from artistic style.12 The core idea is to create a synthetic training modality that mimics the style of the target domain. This is achieved by taking labeled images from a source domain (e.g., photorealistic images) and applying a style transfer network to repaint them in the style of the target artistic domain (e.g., cartoons or sketches). The original classification labels are retained for these new, style-transferred images. The network is then trained on a dual objective: first, to correctly classify objects in both the original and the synthetic source domains, and second, to be *unable* to distinguish which domain an image came from. This second objective is achieved via an adversarial domain confusion loss. By forcing the network to learn features that are modality-indistinguishable, and since the only difference between the two source modalities is artistic style, the network is compelled to factor out style from its representation and learn style-invariant features. A key advantage of this method is its data efficiency; it can achieve significant improvements with as few as ten unlabeled images from the target domain to define the target style, making it ideal for the often data-sparse comics domain.12

This targeted, style-transfer-based approach can be complemented by broader feature-based domain adaptation strategies. These methods are generally categorized into discrepancy-based and adversarial-based approaches.20

* **Discrepancy-based methods** aim to directly minimize a statistical distance metric (e.g., Maximum Mean Discrepancy) between the feature distributions of the source and target domains, pulling the representations closer together in the embedding space.20  
* **Adversarial-based methods**, similar to the domain confusion loss mentioned above, train a domain discriminator to distinguish between source and target features. The main feature extractor is then trained to produce features that "fool" this discriminator, thereby aligning the distributions.20

By applying these UDA techniques, it is possible to train a visual encoder that learns to focus on the fundamental structural and semantic aspects of a character's appearance while ignoring the superficial stylistic rendering. This produces an embedding that is more consistent across different artists and series, forming the first pillar of a robust visual identity signature.

### **2.2. Occlusion-Aware Embeddings for Uninterrupted Identity**

A second major source of visual unreliability is occlusion. In comics, this is often "occlusion-by-design," where crucial parts of a character are deliberately covered by speech bubbles, panel borders, or other characters as a function of layout and storytelling.8 A standard detector, expecting to see a whole object, may fail to detect a partially occluded character or may be confused by the features of the occluding object. An occlusion-aware approach must be able to reason about partial information and disentangle the object of interest from its occluders.

A robust, two-pronged strategy is proposed to handle this challenge:

1. **Explicit Occluder Segmentation:** The first step is to identify and mask out the most common and structured occluders, particularly speech balloons. This can be framed as a semantic segmentation problem. Research has demonstrated that a deep convolutional neural network based on a U-Net architecture can be trained to produce highly accurate, pixel-wise segmentations of speech balloons, including their tails and carriers.8 By applying such a model as a preprocessing step, a precise mask of the occluding balloon can be generated, allowing the subsequent character identification model to know which regions of the image contain unreliable information.  
2. **Part-Based Reasoning with CompositionalNets:** Once occluded regions are identified, the challenge is to perform identification using only the visible parts of the character. This is a task for which CompositionalNets are exceptionally well-suited.15 Unlike standard CNNs that learn holistic templates, CompositionalNets explicitly represent objects as a composition of parts. They can achieve robust classification even when strongly occluded, as their internal voting scheme can infer the presence of the whole object from the spatial configuration of just a few visible parts.

Crucially, recent extensions to CompositionalNets have made them "context-aware".15 These models learn to explicitly disentangle the feature representation of the object from the representation of its context (i.e., the background and occluding objects). This is achieved during training by using bounding box annotations to segment the object from its surroundings. This separation prevents the model from being confused by the context, which is a significant problem in cases of heavy occlusion where the context features can dominate the signal. For example, without this disentanglement, a model might incorrectly associate a character with the features of the speech bubble that is occluding them. By combining explicit occluder segmentation with a context-aware, part-based model, the system can robustly identify characters even when large portions of their body are obscured.

### **2.3. Pose-Invariant Embeddings (PIEs) for Dynamic Characters**

The final major axis of visual variation is pose. Comic characters are defined by their actions, which are often depicted using exaggerated, non-naturalistic, and dynamically foreshortened poses that fall far outside the distribution of poses found in real-world datasets like COCO or MPII.9 This distribution shift causes standard pose estimation and object detection models to fail. The solution is to learn embeddings that are inherently invariant to these changes in viewpoint and articulation.

The concept of **Pose Invariant Embeddings (PIEs)** provides a powerful framework for this task.24 The core principle of PIE is to train a model using multiple views of the same object, with a loss function that encourages the embeddings of all these different views to cluster tightly around a single point in the feature space. This single point becomes the object's pose-invariant descriptor. While originally developed for 3D objects, this concept can be directly adapted to the comics domain by treating the various depictions of a single character in different poses across multiple panels as the "multiple views" for training.

This approach can be further refined by learning dual embeddings. Recent work has shown that performance can be significantly improved by simultaneously learning two separate but related embedding spaces: one for category-level discrimination (e.g., distinguishing a "superhero" from a "civilian") and another for object-identity-level discrimination (e.g., distinguishing "Superman" from "Batman").26 This is highly relevant for comics, where many characters may belong to the same general category but must be uniquely identified.

Furthermore, the challenges of non-humanoid or extreme poses in comics find a useful analogue in the domain of pose estimation for mythological art. For example, the MythPose model was developed to handle pose estimation for multi-limbed figures in Thangka paintings.10 The architectural innovations used in such models to handle non-conventional body plans—such as modules that capture global spatial relationships and handle occlusions from clothing—offer valuable insights for designing visual encoders that can cope with the fantastical and anatomically diverse characters common in comics.

By combining these three strategies—style invariance through domain adaptation, resilience to occlusion through part-based modeling, and robustness to pose variation through invariant embeddings—it is possible to forge a visual feature extractor that produces a significantly more stable and reliable identity signal. This robust visual embedding, however, is not a complete solution. It is the foundation upon which a more powerful, context-aware sequence model can be built.

The process of developing these robust embeddings reveals a deeper truth about the nature of visual information in comics. A truly effective visual representation cannot be a single, monolithic feature vector, as this would inevitably entangle the distinct factors of identity, artistic style, and character pose. Instead, the ideal visual encoder should produce a *decomposed representation*, where these factors are disentangled. The techniques discussed—domain confusion for style, compositional models for occlusion and context, and PIEs for pose—are all methods for forcing a network to learn such a disentangled representation. This suggests an architecture with separate feature vectors or attention heads dedicated to each axis of variation, allowing the model to isolate the core identity-preserving signal from the other, more variable factors.

This decomposition also reveals that the "unreliability" of visual features is not merely noise to be discarded but can itself be a valuable narrative signal. A consistent shift in the "style embedding" across several panels could indicate a flashback sequence drawn by a different artist. A sudden change in a character's "costume embedding" signals a major plot point. A sustained period of high-energy "pose embeddings" denotes an action scene. Therefore, the visual encoder should not simply achieve invariance by throwing this information away. Instead, it should process these variations as separate channels of information to be passed to the sequential narrative model, transforming the encoder from a simple feature extractor into a multi-channel signal processor for the story.

## **Part III: The Narrative as a Sequence: Character Re-Identification as Tracking**

The limitations of static, single-image object detectors in the comics domain point toward a necessary conceptual shift. The task of identifying a character across the panels of a page and the pages of an issue is not a series of independent detection problems. It is a single, unified tracking problem. This section formalizes this paradigm shift, drawing direct analogies from the mature field of multi-object tracking (MOT) in video. By reframing character Re-ID as a tracking task, it becomes possible to leverage powerful, state-of-the-art Transformer-based architectures that are designed to model temporal dependencies and maintain identity consistency over long sequences.

### **3.1. From Static Detections to Dynamic Tracks: A Paradigm Shift**

The analogy between video tracking and comic character Re-ID is direct and powerful. A video is a sequence of frames ordered in time; a comic is a sequence of panels ordered by the narrative reading path. A person moving through a video scene is an object whose identity must be maintained from frame to frame; a character appearing in different panels is an entity whose identity must be maintained from panel to panel. The core challenges of MOT in video have direct counterparts in the comics domain.28

* **Occlusion Handling:** In video, a person may be temporarily hidden by a pillar or another person. In comics, a character may be obscured by a speech bubble or be absent from a panel entirely before reappearing.  
* **ID Switching:** In crowded video scenes, a tracker might confuse two people with similar appearances. In comics, a model might confuse two characters wearing similar costumes or drawn in a similar style.  
* **Trajectory Fragmentation:** A tracker might lose an object and later re-initialize it as a new track. A comic Re-ID system might fail to recognize that a character in panel five is the same one from panel one, creating two separate identities for the same entity.

These challenges are precisely the ones that plague comic character Re-ID. The literature highlights the difficulty of recognizing characters consistently due to complex variations in pose, expression, and context, often exacerbated by the scarcity of data with consistent identity annotations.7 By adopting the "tracking-by-detection" paradigm, which first detects potential objects in each frame (panel) and then associates these detections over time (across panels), we can leverage a rich body of work designed to solve exactly these problems. However, recent advances have moved beyond simple frame-to-frame association to more holistic, sequence-level reasoning, a move enabled by the Transformer architecture.

### **3.2. Transformer Architectures for Temporal Association**

Traditional MOT methods often rely on associating detections between adjacent frames using heuristics based on motion prediction (e.g., Kalman filters) and appearance similarity. This approach is local and greedy, making it prone to error accumulation and unable to easily recover from long-term occlusions. The advent of the Transformer, with its self-attention mechanism, has enabled a new class of trackers that perform global reasoning over an entire sequence of frames, making them far more robust. Several of these architectures are directly adaptable to the comics domain.

* **TrackFormer:** This model formulates MOT as a frame-to-frame set prediction problem using an encoder-decoder Transformer.31 Its key innovation is the concept of "identity preserving track queries." A track query is a learned embedding that represents an active object track. At each new frame (or panel), this query is passed to the decoder, which uses cross-attention to "look at" the features of the current frame and find the detection corresponding to its track. The query then updates its internal state with this new information. This creates an autoregressive process where a character's identity is explicitly carried forward through the sequence via its dedicated track query, providing a powerful mechanism for maintaining identity without relying solely on visual appearance. New characters are initialized using a separate set of "object queries," similar to the DETR object detector.  
* **MOTR (Multiple-Object Tracking with Transformer):** MOTR extends this concept by framing MOT as a "set of sequence prediction" problem.32 Like TrackFormer, it uses "track queries" that are updated frame-by-frame. However, MOTR is designed as a fully end-to-end online system that requires no post-processing steps like Non-Maximum Suppression (NMS) or explicit IoU matching to link tracks. It implicitly learns to model both appearance and motion (narrative flow, in our case) variations over time. The set of track queries is dynamically managed, with new queries being spawned for newborn objects and old queries being pruned when a character exits the narrative for an extended period. This provides a clean and powerful end-to-end solution for generating consistent trajectories.  
* **Global Tracking Transformer (GTR):** While TrackFormer and MOTR are online methods that process frames sequentially, GTR proposes a global, offline approach.33 It takes a whole sequence of frames (e.g., an entire comic page) as input at once. The "trajectory queries" in GTR are simply the object features from one of the frames in the sequence (e.g., the last one). Each query then attends to all detected objects across all frames in the sequence simultaneously to form a complete trajectory in a single forward pass. This global association mechanism is extremely powerful for resolving local ambiguities, as evidence from the entire sequence can be brought to bear on every association decision.

These Transformer-based architectures provide the ideal machinery for solving character Re-ID. Their ability to reason over sequences and maintain stateful representations of each character track is a perfect match for the problem's temporal nature.

### **3.3. Modeling Long-Range Dependencies for Narrative Coherence**

A critical advantage of sequence models, particularly Transformers, is their ability to capture long-range dependencies. In a comic book narrative, a character may be central to a scene, disappear for several pages, and then reappear. A simple appearance-based matching system would almost certainly fail to re-identify this character correctly after such a long "occlusion." The model would have lost the character's track and would likely initialize a new, incorrect identity upon their return.

Transformer-based trackers are inherently designed to mitigate this problem. The self-attention mechanism allows a track query in the current panel to attend to information from all previous panels in the context window. This temporal context is essential for resolving ambiguities. For example, imagine a scene with two characters in similar uniforms. If one of them was established as being at a different location on the previous page, the sequence model can use this long-range contextual information to disambiguate them and perform the correct identity association, even if their visual appearance is nearly identical in the current panel.

This aligns with self-supervised approaches designed specifically for learning from the sequential nature of comics. The ASTERX model, for instance, uses a Transformer encoder to learn representations of comic panels by training it on pretext tasks like predicting a masked-out panel within a sequence and determining if a sequence of panels is in the correct reading order.34 This forces the model to learn the underlying principles of narrative and visual continuity, producing representations that are inherently sensitive to the sequential context. Integrating such a pre-training objective could further enhance the sequence model's ability to understand the narrative flow and make more robust tracking decisions.

The application of these models reveals a powerful parallel between video and comics. The "motion model" used in video tracking, which predicts an object's future location based on its physical velocity and trajectory, finds a direct and compelling analogue in the form of a "narrative model" for comics.28 This narrative model does not predict physical movement but rather a character's likely presence and identity in a future panel based on the established plot, dialogue, and reading order. For instance, a character who declares "I must go to the fortress\!" in one panel has a high probability of appearing in a subsequent panel depicting that fortress. The reading order provides a strong temporal prior, while the dialogue provides a strong semantic prior. A sophisticated sequence model for comics should therefore incorporate a *narrative transition model*. This component, informed by textual cues and panel layout, would function like a Kalman filter for the plot, providing a top-down probabilistic signal to guide the bottom-up visual association process, making the entire system more robust and efficient.

This architectural approach also demonstrates a remarkable alignment with the cognitive processes of human readers. A person reading a comic does not re-identify each character from scratch in every panel using only visual cues. Instead, we maintain a "working memory" of the characters currently active in the scene, their last known state (location, emotional state), and our expectations for their next appearance. As we read sequentially, we update these mental models. The autoregressive nature of trackers like TrackFormer and MOTR, with their evolving "track queries," serves as a computational homologue of this human cognitive process.31 The track query is the model's working memory of "Batman is on the roof," and the cross-attention mechanism is the hypothesis test it performs when it sees a new panel: "Is this the same Batman?" This suggests that the architecture is not merely a black box but employs an interpretable and cognitively plausible mechanism for tracking, reinforcing the validity of the approach.

## **Part IV: A Multi-Modal, Sequence-Aware Framework for Character Identification**

Synthesizing the analyses from the preceding sections—the failure of static detectors, the need for robust visual features, and the power of sequential modeling—this section presents the architectural blueprint for a novel framework designed specifically for character identification and re-identification in comic book narratives. The proposed system abandons the single-modality, single-image paradigm in favor of a multi-stream, sequence-aware approach that fuses visual features, textual cues, and narrative context within a unified Transformer-based architecture. The framework is designed not only to identify characters but to output consistent identity tracklets that can serve as a reliable foundation for downstream narrative analysis.

### **4.1. Architectural Blueprint: A Multi-Stream Approach**

The proposed framework is built around a central fusion and sequence modeling core that processes information from three parallel input streams. This multi-stream design ensures that the model has access to all available signals—visual, textual, and structural—to make robust identity judgments.

1. **Visual Stream:** This stream processes the raw pixel data from each panel. The input is a sequence of panel images. These images are fed into a **domain-adapted visual encoder**, as detailed in Part II. This encoder is not a standard CNN but a specialized network designed to be resilient to the challenges of the comics domain. It would incorporate techniques for style invariance (e.g., via domain adaptation), occlusion awareness (e.g., via part-based modeling), and pose invariance (e.g., using a PIE-based loss). Critically, this encoder would output a *decomposed embedding* for each detected character instance, with separate feature vectors representing identity, style, and pose.  
2. **Textual Stream:** This stream processes the linguistic content of the comic. It begins with an Optical Character Recognition (OCR) module that extracts raw text from detected speech bubbles and caption boxes on the page.35 The extracted text, along with its location, is then passed to a Transformer-based text encoder, such as BERT. This encoder generates contextualized embeddings for each word and sentence, capturing their semantic meaning within the context of the dialogue. This stream provides direct access to character names, pronouns, and plot information that are crucial for identification.  
3. **Narrative Context Stream:** This stream encodes the structural and sequential layout of the comic, which is a vital source of information often ignored by other models. This includes:  
   * **Positional Information:** The absolute and relative bounding box coordinates of panels, detected character instances, and speech bubbles on the page. This spatial information is key for tasks like speaker identification (linking a character to the nearest speech bubble).  
   * **Sequential Information:** The reading order of the panels. This can be determined by rule-based algorithms or learned models and provides the fundamental temporal sequence over which the tracking model operates.19

These three streams are processed for each panel, resulting in a rich, multi-modal representation of the entire comic sequence, ready for fusion and temporal reasoning.

### **4.2. The Fusion Core: Gated Cross-Modal Attention Transformer**

The heart of the framework is a multi-modal, multi-layer Transformer encoder-decoder, architecturally similar to those used in state-of-the-art multi-object tracking and vision-language tasks.38

* **Encoder:** The encoder takes as input the sequence of fused, panel-level representations. For each panel, the visual, textual, and narrative context embeddings are combined. The encoder's self-attention layers then process this entire sequence, building a holistic, context-aware representation of the entire comic page or scene. This allows information to flow freely across panel boundaries, so features in the last panel can influence the interpretation of the first.  
* **Decoder and Gated Cross-Attention:** The decoder is an autoregressive module that performs the actual tracking. It takes as input a set of learned **character track queries**, similar to those in TrackFormer and MOTR.31 Each query represents a unique character identity being tracked. To determine a character's identity and location in the current panel, its track query uses **cross-attention** to attend to the encoder's output, which contains the multi-modal information from the entire sequence.40 This is a crucial mechanism: it allows a character's identity to be determined by non-local evidence. For instance, a visual depiction of a character in panel 2 can be correctly identified as "Superman" because the track query can attend to the word "Superman" appearing in the text of panel 5\.

A key innovation in this fusion process is the integration of **Gated Fusion Mechanisms**.42 Simple fusion methods like concatenation or element-wise addition treat all modalities as equally important at all times. Gated fusion, however, introduces learnable "gates" that dynamically control the information flow from each modality based on the context. For a panel that is heavy on dialogue, the gating mechanism might learn to up-weight the features from the textual stream when updating a character's track query. Conversely, in a silent action sequence, the gates might prioritize the visual stream, particularly the pose embeddings. This allows the model to learn an adaptive fusion strategy, intelligently deciding which modality is the most reliable source of identity information at any given moment in the narrative.

### **4.3. Identity-Aware Contrastive Learning: The Training Objective**

To train this complex architecture end-to-end, a multi-part loss function is required, with a core objective based on cross-modal contrastive learning, inspired by models like CLIP and specialized work in comic character Re-ID.47

The fundamental principle is to learn a shared embedding space where representations of the same character are "close" to each other, and representations of different characters are "far" apart. This is achieved with a contrastive loss, such as InfoNCE, which maximizes the cosine similarity of positive pairs while minimizing the similarity of negative pairs. In this framework, "positive pairs" can be formed in multiple ways:

* **Intra-modal:** Two different visual depictions of the same character in different panels.  
* **Cross-modal:** A visual depiction of a character and a textual mention of their name.  
* **Intra-instance:** The face embedding and body embedding of the same character instance.

This last point is a direct adaptation of the "Identity-Aware" self-supervision method proposed in arXiv:2308.09096.30 That work uses a contrastive loss to pull the embeddings of a character's face and body together, forcing the network to learn a unified representation that encodes identity, regardless of whether the face or the full body is visible.50 This principle is extended here to the multi-modal case. The model will be trained to learn that the visual embedding of Superman's face in panel 1, the visual embedding of his full body in panel 2, and the textual embedding of the name "Clark" in panel 3 should all be close together in the shared embedding space, and far from all embeddings related to Batman. This multi-faceted contrastive objective forces the model to learn a truly robust, multi-modal concept of character identity.

### **4.4. The Output Layer: Probabilistic Identity Tracklets**

The final output of the proposed framework for a given comic book is not a list of independent bounding boxes with class labels. Instead, it is a set of **probabilistic identity tracklets**. Each tracklet corresponds to a single, unique character identity discovered and tracked by the model. A tracklet contains:

* A unique track ID (e.g., char\_001, char\_002).  
* A sequence of all bounding box detections associated with this identity across all panels and pages.  
* A confidence score for each association.  
* Potentially, a summary of the character's state, derived from the learned embeddings (e.g., dominant pose types, associated textual keywords).

This output format is fundamentally more useful than standard detection output. It directly represents the narrative persistence of each character, making it immediately usable for any downstream task that requires consistent identity tracking. By framing the output as a set of trajectories, the problem of error propagation through ID switches is explicitly addressed and minimized by the model's architecture and training objective.

This architecture implicitly learns a "character-centric" representation of the narrative. Instead of processing the comic linearly, panel by panel, the Transformer's cross-attention mechanism effectively reorganizes the story around each character. For the "Superman" track query, the model learns to place high attention on all visual depictions of Superman and all textual mentions of his name or aliases across the entire sequence. It is, in effect, performing a parallel, character-focused reading, compiling a complete "dossier" of all relevant multi-modal evidence for each character before making a final, holistic identity judgment. This makes the system exceptionally robust to local ambiguities, as a blurry or occluded depiction in one panel can be confidently identified using clear textual evidence from two pages later.

Furthermore, the gated fusion mechanism provides more than just an engineering improvement. It is a mechanism for the model to learn and represent the concept of *narrative focus*. The sequence of learned modality weights from the gates for a character's tracklet can be interpreted as a "modality importance" signature over time. This signature reflects the character's role in each scene. A character whose gates consistently prioritize the textual stream is likely a narrator or a dialogue-heavy character, while one whose gates prioritize the visual stream is likely an "action hero." This opens up a novel avenue for computational literary analysis, where these learned gate weights could be used to automatically classify scenes (e.g., "dialogue," "action," "exposition") or even to characterize an author's or artist's unique storytelling style based on their typical balance of visual and textual information. The model learns not just *who* the characters are, but *how* their story is being told.

## **Part V: Ground Truth, Evaluation, and Future Directions**

The proposal of a novel computational framework, no matter how theoretically sound, is only meaningful if it can be practically implemented and rigorously evaluated. This final section addresses the crucial prerequisites of data and metrics, outlining a strategy for creating the necessary ground-truth annotations and advocating for the adoption of evaluation standards that measure what truly matters: narrative coherence and tracking stability. Finally, it looks ahead to the open challenges and future research directions that this framework would enable.

### **5.1. A Strategy for Scalable, Multi-Modal Annotation**

The single greatest bottleneck in the field of computational comic analysis remains the lack of large-scale, high-quality, and consistently annotated datasets.1 Creating the ground truth required to train and evaluate the proposed multi-modal sequence model is a significant undertaking. A purely manual annotation process is prohibitively slow and expensive. Therefore, a more scalable, semi-automated, human-in-the-loop strategy is essential.

1. **Bootstrapping with Existing Models:** The process can be bootstrapped by using existing, off-the-shelf models to generate a first pass of noisy annotations. Standard object detectors can propose initial bounding boxes for characters and faces. OCR models can extract text. Simple heuristics, such as linking a speech bubble to the nearest character, can provide initial speaker identification labels.17 While these initial labels will contain many errors, they provide a strong starting point.  
2. **Human-in-the-Loop Correction:** These machine-generated annotations are then loaded into a sophisticated annotation tool, such as CVAT or a custom-built interface.4 The task for human annotators is not to create annotations from scratch but to *correct* the errors made by the automated systems. This workflow—correcting bounding boxes, re-assigning incorrect speaker labels, and merging fragmented character tracks—is significantly faster and less cognitively demanding than manual annotation.  
3. **Focus on Linked Primitives:** The annotation schema should be built upon a foundation of simple, linked primitives. The core annotated elements are: (1) bounding boxes for characters and faces, (2) transcribed text from bubbles and captions, and (3) directed links for speaker identification (from text box to character box).19 The most critical and laborious task is assigning a *consistent identity label* to all character primitives belonging to the same individual across an entire narrative sequence. The data structure described in the comic\_char\_reid repository, which uses JSON files to link character IDs to lists of their corresponding face and body instances across panels, provides an excellent template for this.50  
4. **Sequence-First Annotation:** To ensure long-range identity consistency is captured correctly in the ground truth, the annotation process should be organized around narrative sequences (e.g., a complete scene, chapter, or issue) rather than on a page-by-page basis. This forces annotators to resolve long-range re-identification challenges, creating the high-quality, long-term tracking data necessary to train and evaluate the proposed sequence model.

### **5.2. Metrics for Narrative Coherence and Tracking Stability**

Just as standard object detectors are the wrong tool for the job, standard detection metrics are the wrong measure of success. Metrics like mean Average Precision (mAP) are insufficient because they evaluate each detection in isolation. A model could achieve a high mAP by correctly detecting every character instance but assigning each one a different ID, completely failing at the core task of Re-ID. To properly evaluate the proposed framework, the field must adopt metrics from the multi-object tracking community that are explicitly designed to measure temporal consistency and penalize identity failures.

* **CLEAR MOT Metrics:** This suite of metrics provides the foundational tools for tracking evaluation.29 The most important of these is the **Multiple Object Tracking Accuracy (MOTA)**. MOTA combines three error types into a single, intuitive score: false positives (spurious detections), false negatives (missed objects), and, most critically, **ID switches** (misidentifying a tracked object as a different object). By directly penalizing ID switches, MOTA measures the model's ability to maintain stable trajectories.  
* **HOTA (Higher Order Tracking Accuracy):** A more recent metric, HOTA, improves upon MOTA by explicitly decoupling the evaluation of detection, association, and localization accuracy.29 It provides a more balanced and comprehensive assessment of tracking performance, making it a strong candidate for the primary evaluation metric.  
* **IDF1 Score:** While MOTA and HOTA provide an overall score, the IDF1 score is used to specifically measure association performance, ignoring detection quality. It computes the F1-score on identity assignments, providing a clear signal on how well the model performs the Re-ID task itself, independent of its ability to detect the characters in the first place.52  
* **Long-Range Stability Metrics:** To specifically test the model's ability to handle long-range dependencies, novel evaluation protocols could be developed. For example, the concept of **Path Consistency Loss (PCL)**, originally proposed as a self-supervised training objective, could be adapted as an evaluation metric.52 This would involve evaluating the tracker on sequences where intermediate panels have been removed. A robust tracker should maintain consistent identity assignments across these temporal gaps, and its ability to do so could be quantified as a measure of long-range stability.

The choice of evaluation metric is not a minor detail; it actively shapes the direction of research. The community's historical reliance on mAP has incentivized the development of better static detectors but has failed to reward models that maintain temporal identity. A formal shift in benchmark standards to prioritize MOTA, HOTA, and IDF1 would send a clear signal to researchers, incentivizing the development of sequence-based models that solve the problem of narrative coherence and directly address the critical issue of ID switches. It is a matter of defining success correctly to drive progress in the right direction.

### **5.3. Open Challenges and the Path Forward**

The proposed framework represents a significant step forward, but it is not a final solution. Its implementation and evaluation will highlight a new set of open challenges and point the way toward future research directions.

* **Handling Narrative Inconsistencies ("Retcons"):** Comic book narratives are famous for their "retroactive continuity," or retcons, where a character's design, backstory, or even identity can be fundamentally altered mid-series. How should a model handle a ground truth that is not static? This may require models that can detect and adapt to these author-driven shifts in the underlying data distribution.  
* **Zero-Shot and Open-World Re-ID:** The proposed framework relies on being trained on a set of known characters. A truly scalable system must be able to identify and track characters it has never seen before (zero-shot) and dynamically add new, previously unknown characters to its set of tracked identities as they are introduced in the story (open-world).  
* **Integration with External Knowledge Graphs:** The model's understanding of character identity is learned solely from the comic itself. Performance could be dramatically improved by fusing its learned representations with external, structured knowledge bases, such as fan-curated wikis. These knowledge graphs contain explicit information about character aliases (e.g., "Clark Kent" is "Superman"), family relationships, and team affiliations, which could provide powerful top-down constraints to resolve ambiguities during the tracking process.  
* **Beyond Identification to Narrative Understanding:** The ultimate goal is not just to identify characters but to understand the story. The stable, multi-modal character tracklets produced by this framework are the ideal foundation for a new generation of high-level narrative analysis tools. With reliable character tracking, researchers can begin to tackle complex tasks like automatically generating chapter-level plot summaries, mapping the evolution of character relationships over time, analyzing character development arcs based on their actions and dialogue, and even studying authorial style through patterns in narrative structure.

The very process of creating the large-scale ground truth needed for this research is, in itself, a form of deep literary analysis. The ambiguities and edge cases that challenge human annotators—distinguishing between a flashback and the present, identifying a character in disguise, determining if two visually distinct entities are the same person after a transformation—are the very narrative devices that literary scholars study. The detailed annotation guidelines required to consistently resolve these cases would effectively constitute a formal, operationalized theory of character identity in the comics medium. The resulting dataset would be more than just training data for a machine learning model; it would be a valuable scholarly artifact, a structured representation of the narrative's complex identity logic. Analyzing the points of highest inter-annotator disagreement could even serve as a method for automatically pinpointing the most narratively complex and interesting moments in a comic series, bridging the gap between computational analysis and the digital humanities.

#### **Works cited**

1. Comics Datasets Framework: Mix of Comics datasets for detection ..., accessed November 3, 2025, [https://arxiv.org/pdf/2407.03540](https://arxiv.org/pdf/2407.03540)  
2. (PDF) Comics Datasets Framework: Mix of Comics datasets for detection benchmarking, accessed November 3, 2025, [https://www.researchgate.net/publication/382065236\_Comics\_Datasets\_Framework\_Mix\_of\_Comics\_datasets\_for\_detection\_benchmarking](https://www.researchgate.net/publication/382065236_Comics_Datasets_Framework_Mix_of_Comics_datasets_for_detection_benchmarking)  
3. Comics Datasets Framework: Mix of Comics datasets for detection benchmarking \- arXiv, accessed November 3, 2025, [https://arxiv.org/abs/2407.03540](https://arxiv.org/abs/2407.03540)  
4. Mix of Comics datasets for detection benchmarking \- arXiv, accessed November 3, 2025, [https://arxiv.org/html/2407.03540v1](https://arxiv.org/html/2407.03540v1)  
5. Why Object Detectors Fail: Investigating the ... \- CVF Open Access, accessed November 3, 2025, [https://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Miller\_Why\_Object\_Detectors\_Fail\_Investigating\_the\_Influence\_of\_the\_Dataset\_CVPRW\_2022\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Miller_Why_Object_Detectors_Fail_Investigating_the_Influence_of_the_Dataset_CVPRW_2022_paper.pdf)  
6. Diagnosing Error in Object Detectors \- Derek Hoiem, accessed November 3, 2025, [https://dhoiem.cs.illinois.edu/publications/eccv2012\_detanalysis\_derek.pdf](https://dhoiem.cs.illinois.edu/publications/eccv2012_detanalysis_derek.pdf)  
7. Identifying Visual Depictions of Animate Entities in Narrative Comics: An Annotation Study \- ACL Anthology, accessed November 3, 2025, [https://aclanthology.org/2023.wnu-1.14.pdf](https://aclanthology.org/2023.wnu-1.14.pdf)  
8. Deep CNN-based Speech Balloon Detection and Segmentation for ..., accessed November 3, 2025, [https://arxiv.org/abs/1902.08137](https://arxiv.org/abs/1902.08137)  
9. POSE-ID-on—A Novel Framework for Artwork Pose Clustering \- MDPI, accessed November 3, 2025, [https://www.mdpi.com/2220-9964/10/4/257](https://www.mdpi.com/2220-9964/10/4/257)  
10. MythPose: Enhanced Detection of Complex Poses in Thangka Figures, accessed November 3, 2025, [https://www.mdpi.com/1424-8220/25/16/4983](https://www.mdpi.com/1424-8220/25/16/4983)  
11. Artistic Object Recognition by Unsupervised Style Adaptation \- ResearchGate, accessed November 3, 2025, [https://www.researchgate.net/publication/333427305\_Artistic\_Object\_Recognition\_by\_Unsupervised\_Style\_Adaptation](https://www.researchgate.net/publication/333427305_Artistic_Object_Recognition_by_Unsupervised_Style_Adaptation)  
12. Artistic Object Recognition by Unsupervised Style Adaptation, accessed November 3, 2025, [https://people.cs.pitt.edu/\~kovashka/thomas\_kovashka\_accv2018.pdf](https://people.cs.pitt.edu/~kovashka/thomas_kovashka_accv2018.pdf)  
13. BAM\! The Behance Artistic Media Dataset for Recognition Beyond Photography | Request PDF \- ResearchGate, accessed November 3, 2025, [https://www.researchgate.net/publication/322059577\_BAM\_The\_Behance\_Artistic\_Media\_Dataset\_for\_Recognition\_Beyond\_Photography](https://www.researchgate.net/publication/322059577_BAM_The_Behance_Artistic_Media_Dataset_for_Recognition_Beyond_Photography)  
14. 'Do's and 'Don't's of speech bubbles by DrZime on DeviantArt, accessed November 3, 2025, [https://www.deviantart.com/drzime/journal/Do-s-and-Don-t-s-of-speech-bubbles-784479859](https://www.deviantart.com/drzime/journal/Do-s-and-Don-t-s-of-speech-bubbles-784479859)  
15. Robust Object Detection Under Occlusion With ... \- CVF Open Access, accessed November 3, 2025, [https://openaccess.thecvf.com/content\_CVPR\_2020/papers/Wang\_Robust\_Object\_Detection\_Under\_Occlusion\_With\_Context-Aware\_CompositionalNets\_CVPR\_2020\_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Robust_Object_Detection_Under_Occlusion_With_Context-Aware_CompositionalNets_CVPR_2020_paper.pdf)  
16. Full article: Deep learning-based human pose estimation towards artworks classification, accessed November 3, 2025, [https://www.tandfonline.com/doi/full/10.1080/24751839.2024.2331866](https://www.tandfonline.com/doi/full/10.1080/24751839.2024.2331866)  
17. CoMix: A Comprehensive Benchmark for Multi-Task Comic ..., accessed November 3, 2025, [https://proceedings.neurips.cc/paper\_files/paper/2024/file/fe79898dcf078ec54b6feeea10ebb751-Supplemental-Datasets\_and\_Benchmarks\_Track.pdf](https://proceedings.neurips.cc/paper_files/paper/2024/file/fe79898dcf078ec54b6feeea10ebb751-Supplemental-Datasets_and_Benchmarks_Track.pdf)  
18. \[2407.03550\] CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding, accessed November 3, 2025, [https://arxiv.org/abs/2407.03550](https://arxiv.org/abs/2407.03550)  
19. CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding \- arXiv, accessed November 3, 2025, [https://arxiv.org/html/2407.03550v1](https://arxiv.org/html/2407.03550v1)  
20. Feature Based Methods in Domain Adaptation for Object ... \- arXiv, accessed November 3, 2025, [https://arxiv.org/pdf/2412.17325](https://arxiv.org/pdf/2412.17325)  
21. Progressive Domain Adaptation for Object ... \- CVPR Workshop, accessed November 3, 2025, [https://vision4allseason.net/wp-content/uploads/2019/06/cvpr19\_workshop\_da\_detection\_camera\_ready.pdf](https://vision4allseason.net/wp-content/uploads/2019/06/cvpr19_workshop_da_detection_camera_ready.pdf)  
22. Active Domain Adaptation with False Negative Prediction for Object Detection \- CVF Open Access, accessed November 3, 2025, [https://openaccess.thecvf.com/content/CVPR2024/papers/Nakamura\_Active\_Domain\_Adaptation\_with\_False\_Negative\_Prediction\_for\_Object\_Detection\_CVPR\_2024\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Nakamura_Active_Domain_Adaptation_with_False_Negative_Prediction_for_Object_Detection_CVPR_2024_paper.pdf)  
23. Different levels of difficulty for speech balloon and comic character... \- ResearchGate, accessed November 3, 2025, [https://www.researchgate.net/figure/Different-levels-of-difficulty-for-speech-balloon-and-comic-character-association-From\_fig2\_308862154](https://www.researchgate.net/figure/Different-levels-of-difficulty-for-speech-balloon-and-comic-character-association-From_fig2_308862154)  
24. PIEs: Pose Invariant Embeddings \- CVF Open Access, accessed November 3, 2025, [https://openaccess.thecvf.com/content\_CVPR\_2019/papers/Ho\_PIEs\_Pose\_Invariant\_Embeddings\_CVPR\_2019\_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2019/papers/Ho_PIEs_Pose_Invariant_Embeddings_CVPR_2019_paper.pdf)  
25. PIEs: Pose Invariant Embeddings, accessed November 3, 2025, [http://www.svcl.ucsd.edu/projects/OOWL/CVPR2019\_PIE.html](http://www.svcl.ucsd.edu/projects/OOWL/CVPR2019_PIE.html)  
26. Dual Pose-invariant Embeddings: Learning Category and Object-specific Discriminative Representations for Recognition and Retriev \- CVF Open Access, accessed November 3, 2025, [https://openaccess.thecvf.com/content/CVPR2024/papers/Sarkar\_Dual\_Pose-invariant\_Embeddings\_Learning\_Category\_and\_Object-specific\_Discriminative\_Representations\_for\_CVPR\_2024\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Sarkar_Dual_Pose-invariant_Embeddings_Learning_Category_and_Object-specific_Discriminative_Representations_for_CVPR_2024_paper.pdf)  
27. Dual Pose-invariant Embeddings: Learning Category and Object-specific Discriminative Representations for Recognition and Retrieval \- arXiv, accessed November 3, 2025, [https://arxiv.org/html/2403.00272v1](https://arxiv.org/html/2403.00272v1)  
28. Multiple Object Tracking (MOT): Methods & Latest Advances \- Roboflow Blog, accessed November 3, 2025, [https://blog.roboflow.com/multiple-object-tracking/](https://blog.roboflow.com/multiple-object-tracking/)  
29. (PDF) Evaluating multiple object tracking performance: The CLEAR ..., accessed November 3, 2025, [https://www.researchgate.net/publication/26523191\_Evaluating\_multiple\_object\_tracking\_performance\_The\_CLEAR\_MOT\_metrics](https://www.researchgate.net/publication/26523191_Evaluating_multiple_object_tracking_performance_The_CLEAR_MOT_metrics)  
30. (PDF) Identity-Aware Semi-Supervised Learning for Comic Character Re-Identification, accessed November 3, 2025, [https://www.researchgate.net/publication/373246266\_Identity-Aware\_Semi-Supervised\_Learning\_for\_Comic\_Character\_Re-Identification](https://www.researchgate.net/publication/373246266_Identity-Aware_Semi-Supervised_Learning_for_Comic_Character_Re-Identification)  
31. timmeinhardt/trackformer: Implementation of "TrackFormer ... \- GitHub, accessed November 3, 2025, [https://github.com/timmeinhardt/trackformer](https://github.com/timmeinhardt/trackformer)  
32. MOTR: End-to-End Multiple-Object Tracking with Transformer, accessed November 3, 2025, [https://www.ecva.net/papers/eccv\_2022/papers\_ECCV/papers/136870648.pdf](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136870648.pdf)  
33. Global Tracking Transformers \- Vladlen Koltun, accessed November 3, 2025, [https://vladlen.info/papers/GTR.pdf](https://vladlen.info/papers/GTR.pdf)  
34. Drawing Insights: Sequential Representation ... \- BMVA Archive, accessed November 3, 2025, [https://bmva-archive.org.uk/bmvc/2024/papers/Paper\_650/paper.pdf](https://bmva-archive.org.uk/bmvc/2024/papers/Paper_650/paper.pdf)  
35. COMICORDA: Dialogue Act Recognition in Comic Books \- ACL Anthology, accessed November 3, 2025, [https://aclanthology.org/2024.lrec-main.316/](https://aclanthology.org/2024.lrec-main.316/)  
36. Multimodal Persona Based Generation of Comic Dialogs \- CSE, IIT Delhi, accessed November 3, 2025, [https://www.cse.iitd.ac.in/\~mausam/papers/acl23a.pdf](https://www.cse.iitd.ac.in/~mausam/papers/acl23a.pdf)  
37. CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding \- arXiv, accessed November 3, 2025, [https://arxiv.org/html/2407.03550v2](https://arxiv.org/html/2407.03550v2)  
38. Everything at Once – Multi-modal Fusion Transformer for Video Retrieval, accessed November 3, 2025, [https://mitibmwatsonailab.mit.edu/research/blog/everything-at-once-multi-modal-fusion-transformer-for-video-retrieval/](https://mitibmwatsonailab.mit.edu/research/blog/everything-at-once-multi-modal-fusion-transformer-for-video-retrieval/)  
39. Beyond Text: Architecting Multimodal Transformers for Vision — Language — Audio Fusion | by James Fahey | Sep, 2025 | Medium, accessed November 3, 2025, [https://medium.com/@fahey\_james/beyond-text-architecting-multimodal-transformers-for-vision-language-audio-fusion-c3d6e6e55e54](https://medium.com/@fahey_james/beyond-text-architecting-multimodal-transformers-for-vision-language-audio-fusion-c3d6e6e55e54)  
40. Cross attention for Text and Image Multimodal data fusion \- Stanford ..., accessed November 3, 2025, [https://web.stanford.edu/class/cs224n/final-reports/256711050.pdf](https://web.stanford.edu/class/cs224n/final-reports/256711050.pdf)  
41. Language-Driven Cross-Attention for Visible–Infrared Image Fusion Using CLIP \- PMC \- NIH, accessed November 3, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12390620/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12390620/)  
42. Dual-Gated Fusion Methods \- Emergent Mind, accessed November 3, 2025, [https://www.emergentmind.com/topics/dual-gated-fusion-method](https://www.emergentmind.com/topics/dual-gated-fusion-method)  
43. \[PDF\] Gated Multimodal Units for Information Fusion | Semantic ..., accessed November 3, 2025, [https://www.semanticscholar.org/paper/Gated-Multimodal-Units-for-Information-Fusion-Arevalo-Solorio/37e3721940352df07faebd87620732c05b458985](https://www.semanticscholar.org/paper/Gated-Multimodal-Units-for-Information-Fusion-Arevalo-Solorio/37e3721940352df07faebd87620732c05b458985)  
44. Gated Multimodal Graph Learning for Personalized Recommendation \- arXiv, accessed November 3, 2025, [https://arxiv.org/html/2506.00107v1](https://arxiv.org/html/2506.00107v1)  
45. Gated Recursive Fusion: A Stateful Approach to Scalable Multimodal Transformers \- arXiv, accessed November 3, 2025, [https://arxiv.org/abs/2507.02985](https://arxiv.org/abs/2507.02985)  
46. Group Gated Fusion on Attention-Based Bidirectional Alignment for Multimodal Emotion Recognition \- ISCA Archive, accessed November 3, 2025, [https://www.isca-archive.org/interspeech\_2020/liu20b\_interspeech.pdf](https://www.isca-archive.org/interspeech_2020/liu20b_interspeech.pdf)  
47. CLIP: Contrastive Language-Image Pre-Training, accessed November 3, 2025, [https://viso.ai/deep-learning/clip-machine-learning/](https://viso.ai/deep-learning/clip-machine-learning/)  
48. An Investigation of the Domain Gap in CLIP-Based Person Re ..., accessed November 3, 2025, [https://www.mdpi.com/1424-8220/25/2/363](https://www.mdpi.com/1424-8220/25/2/363)  
49. Identity-Aware Semi-Supervised Learning for Comic Character Re-Identification \- arXiv, accessed November 3, 2025, [https://arxiv.org/abs/2308.09096](https://arxiv.org/abs/2308.09096)  
50. gsoykan/comic\_char\_reid: Official Repository for "Identity ... \- GitHub, accessed November 3, 2025, [https://github.com/gsoykan/comic\_char\_reid](https://github.com/gsoykan/comic_char_reid)  
51. A Comprehensive Gold Standard and Benchmark for Comics Text Detection and Recognition \- ResearchGate, accessed November 3, 2025, [https://www.researchgate.net/publication/366789766\_A\_Comprehensive\_Gold\_Standard\_and\_Benchmark\_for\_Comics\_Text\_Detection\_and\_Recognition](https://www.researchgate.net/publication/366789766_A_Comprehensive_Gold_Standard_and_Benchmark_for_Comics_Text_Detection_and_Recognition)  
52. Self-Supervised Multi-Object Tracking with Path Consistency \- arXiv, accessed November 3, 2025, [https://arxiv.org/html/2404.05136v1](https://arxiv.org/html/2404.05136v1)