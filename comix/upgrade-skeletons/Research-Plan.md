

# **A Framework for Multi-Modal, Sequential, and Compositional Embeddings for Comic Book Analysis**

## **Section 1: Introduction: The Imperative for Narrative-Aware Embeddings in Comic Analysis**

### **1.1 The "Invisible Art" as a Grand Challenge for Multimodal AI**

The medium of comic books, eloquently described by Scott McCloud as an "invisible art," represents a unique and formidable grand challenge for contemporary artificial intelligence.1 Comics are a form of "sequential art," where narrative is constructed not just within individual panels but crucially in the space

*between* them.3 This inter-panel space, the "gutter," requires the reader to perform a cognitive leap known as "closure"—inferring action, the passage of time, and causal relationships from a sequence of discrete, static images.1 This fundamental principle of comic storytelling highlights a significant frontier for AI. While current large multimodal models (LMMs) have demonstrated remarkable capabilities in tasks involving single, disconnected artifacts like image captioning or visual question answering, they fundamentally struggle with the inferential, sequential, and compositional grammar that defines the comic medium.5

Direct evidence of this limitation is starkly provided by recent academic benchmarks. On the ComicsPAP benchmark, which features a "Pick-a-Panel" task requiring a model to select the correct missing panel in a sequence, state-of-the-art LMMs perform at a level near random chance.8 This failure underscores that these powerful models, despite their vast training, lack a deep understanding of the sequential and contextual dependencies that are second nature to a human reader. The challenge is not merely multimodal; it is one of modeling

*inferred temporality*. Unlike video, where time is continuous and can be explicitly sampled into frames, comics present a sequence of deliberately chosen, "motivated" moments.3 A successful model cannot simply adapt architectures from video action recognition, which often rely on explicit motion cues like optical flow.10 Instead, it must learn to comprehend a unique "narrative physics" where the transition from one panel to the next is governed by storytelling conventions, character intent, and logical causality rather than the linear passage of physical time. This necessitates a move beyond recognizing content to understanding context and consequence.

### **1.2 Limitations of Existing Paradigms**

Current computational approaches to comic analysis are insufficient to tackle this challenge. The predominant paradigms, such as page-level analysis or the application of general-purpose vision-language models (VLMs) like CLIP, are fundamentally misaligned with the granular, sequential nature of the medium. These methods often treat comic pages as composite images or collections of independent captioned pictures, thereby ignoring the deliberate narrative architecture woven by the creator through panel sequence, layout, and composition.11 This approach leads to a "semantic flattening" of the rich narrative tapestry.

For instance, a standard VLM embedding might accurately identify the content of a panel as "a man in a red cape and blue suit".12 However, it would likely miss the crucial narrative context: that this is Superman, that the preceding panels have shown him in a losing battle, that his posture signifies exhaustion, and that the panel's composition—a low-angle shot looking up at a triumphant villain—is a cinematographic choice designed to emphasize his defeat and vulnerability. The embedding captures the "what" but completely misses the "how" and "why" of the story. Similarly, retrieval systems based on these embeddings can find visually similar images but fail to grasp narrative similarity, unable to distinguish a character's moment of triumph from their moment of despair if the visual elements are superficially alike.11 To truly understand comics, a model must move beyond content recognition to narrative comprehension.

### **1.3 Research Objectives and Proposed Framework Overview**

The primary objective of this report is to theorize and specify a comprehensive framework for creating **multi-modal, sequential, panel-level embeddings** that capture the intricate interplay of visual, textual, and narrative elements. This endeavor requires a paradigm shift from treating panels as isolated data points to understanding them as interconnected nodes in a complex narrative structure.

To achieve this, a novel hierarchical methodology is proposed, structured around four key stages:

1. **Deconstruction and Feature Extraction:** Each panel is first atomized into its constituent modalities. This involves using advanced VLMs to extract distinct vector representations for the panel's visual content, its complete textual information (dialogue, narration, onomatopoeia), and, critically, its compositional structure (e.g., shot scale, character positioning).  
2. **Intra-Panel Fusion:** These disparate modal vectors are then fused into a single, coherent, and rich panel embedding. This process moves beyond simple concatenation to employ sophisticated attention-based mechanisms, allowing the modalities to inform and contextualize one another dynamically.  
3. **Inter-Panel Sequencing:** The sequence of fused panel embeddings is processed by a powerful sequential encoder, such as a Transformer. This stage is designed to explicitly model the narrative flow and contextual relationships between panels, allowing each panel's representation to be aware of its place in the story.  
4. **Hierarchical Aggregation:** Finally, the context-aware panel embeddings are aggregated into higher-level representations. An attention-based mechanism combines panel embeddings into a page-level embedding, and subsequently, page embeddings are aggregated into a single, holistic comic-level embedding, enabling multi-scale analysis.

This report will systematically detail the architectural considerations, data processing strategies, and evaluation methodologies required to realize this framework, providing a technical blueprint for advancing the state of the art in the computational understanding of comics.

## **Section 2: Literature Review: Foundations of Sequential and Multimodal Representation**

A robust framework for comic analysis must be built upon a synthesis of prior work, both within the specialized domain of computational comics research and from adjacent fields that have tackled similar challenges in multimodal and sequential data.

### **2.1 The Evolution of Computational Comics Analysis**

The field of computational comics analysis has progressed through distinct phases, moving from foundational structural parsing to more complex narrative understanding.5

Early Stages: Structural Decomposition  
Initial research focused on the essential, yet preliminary, task of structural decomposition. This involved developing methods for fundamental computer vision tasks tailored to the comic book page, such as panel extraction, speech balloon segmentation, and Optical Character Recognition (OCR) for text extraction.11 While these tools are critical prerequisites for any deeper analysis—forming the basis for converting a scanned page into machine-readable components—they do not, in themselves, provide any narrative or semantic understanding. They answer "where are the panels?" but not "what do the panels mean?"  
The Rise of Multimodality: Connecting Vision and Language  
The next wave of research began to bridge the gap between the visual and textual elements. This phase saw the emergence of tasks that required a joint understanding of both modalities. Prominent examples include speaker identification (linking a speech balloon to the character speaking it), character re-identification (tracking the same character across different panels despite stylistic variations), and multimodal sentiment analysis (determining the emotional tone of a panel from both facial expressions and dialogue).17 These tasks represent a significant step forward, demanding models that can correlate specific visual objects with specific textual content.  
The Sequential Frontier: Modeling Narrative Flow  
More recently, the research community has shifted its focus to the ultimate challenge: understanding the sequential nature of comics. This has been driven by the development of sophisticated benchmarks and models designed to probe narrative comprehension.

* **COMICS Dataset and Closure Tasks:** The COMICS dataset was among the first to formalize the concept of narrative understanding as a machine learning task.21 It introduced "closure tasks," directly inspired by McCloud's theory, where a model is given a sequence of context panels and must complete it. These tasks, such as visual-cloze (picking the correct next panel from a set of options) and text-cloze (picking the correct text for a panel), provided the first quantitative measures of a model's ability to grasp panel-to-panel relationships.22  
* **ComicsPAP and CoMix Benchmarks:** Building on this foundation, modern benchmarks like ComicsPAP and CoMix have exposed the profound limitations of current-generation LMMs.8 ComicsPAP uses a "Pick-A-Panel" framework to test skills like narrative anticipation and multi-frame reasoning, revealing that even massive models perform poorly.21 CoMix provides a comprehensive suite of multi-task evaluations, including object detection, reading order determination, and dialogue generation, creating a robust standard for assessing model capabilities across diverse comic styles.17  
* **ASTERX Model:** A key architectural precedent is the ASTERX model, which presents a self-supervised method for learning sequential panel representations.24 ASTERX employs a Transformer encoder to process sequences of panel features. Crucially, it is trained on unlabeled data using objectives inspired by masked language modeling:  
  **masked panel retrieval** (predicting a masked panel from its context) and **order classification** (determining if two panel sequences are in the correct chronological order).24 This work demonstrates the feasibility of learning meaningful sequential representations directly from the structure of comics without requiring extensive manual annotation.

A significant gap remains, however. Even advanced sequential models like ASTERX typically operate on a pre-determined, one-dimensional sequence of panels. This linearization process discards the rich two-dimensional *spatial grammar* of the comic page. Research in visual language theory by scholars like Neil Cohn has shown that the layout of panels—their size, shape, and placement—is not arbitrary but is a crucial part of the narrative language that guides the reader's eye and controls the story's pacing and rhythm.25 The conventional left-to-right, top-to-bottom "Z-path" of reading is frequently violated by more complex layouts involving panel blockage or perceptual grouping.25 By converting the page to a 1D sequence before analysis, current models lose this vital compositional information. A truly sophisticated model must therefore learn not only the "what" (content) and "when" (sequence) but also the "how" (navigational path) of the comic narrative.

### **2.2 Learnings from Adjacent Fields**

To architect a solution, it is essential to draw upon insights from more mature fields that have addressed analogous problems in sequential and multimodal data.

Video Action Recognition  
The field of video action recognition offers a rich history of modeling spatio-temporal data. The evolution of architectures in this domain provides a clear roadmap for handling sequential information.

* **Temporal Modeling:** Early successful models like **two-stream networks** explicitly separated the problem into a spatial stream (for appearance) and a temporal stream (for motion, typically using optical flow).10 This highlighted the importance of treating motion as a first-class feature. Subsequent  
  **3D Convolutional Networks (3D CNNs)** integrated spatial and temporal feature extraction into a single network by using 3D kernels.10 Most recently,  
  **Vision Transformers (ViTs)** have become dominant, demonstrating an exceptional ability to model long-range spatio-temporal dependencies within video clips.27 The success of Transformers in capturing long-range context makes them a prime architectural candidate for encoding panel sequences, where an event in an early panel can be critical for understanding a much later one.

Visual Storytelling (VIST)  
The VIST task, which involves generating a coherent, multi-sentence story from a sequence of images, is a close cousin to comic analysis.31 This field provides two crucial insights.

* **Narrative Coherence:** VIST models are explicitly designed to maintain narrative context over long sequences and to generate imaginative or inferential text that goes beyond a literal description of the images.33 This directly parallels the need to understand the "closure" in comics, where the narrative is more than the sum of its visible parts.  
* **Hierarchical Architectures:** A common and powerful architectural pattern in VIST and document analysis is the **Hierarchical Attention Network (HAN)**.32 HANs construct a document representation by first creating sentence vectors from word vectors, and then aggregating those sentence vectors into a final document vector. At each stage, an attention mechanism learns to weight the more important components (important words in a sentence, important sentences in a document) more heavily.34 This hierarchical, attention-based aggregation provides a direct and proven blueprint for the proposed strategy of building page embeddings from panel embeddings, and comic embeddings from page embeddings.

Multimodal Fusion Techniques  
The challenge of combining information from different modalities is central to this project. Fields like multimodal sentiment analysis have explored a range of fusion strategies.37 Early methods often relied on  
**early fusion** (concatenating raw features) or **late fusion** (combining decisions from separate unimodal classifiers).5 However, more advanced techniques have emerged that enable deeper interaction between modalities.

**Cross-modal attention** is a particularly powerful mechanism, allowing one modality to "query" another and attend to its most relevant features.39 For example, a text representation can be used to guide the visual feature extractor to focus on specific objects mentioned in the text. This principle of learned, dynamic interaction justifies the proposed fusion mechanism in the panel embedding architecture.

## **Section 3: A Novel Hierarchical Architecture for Multimodal Sequential Panel Embeddings**

This section details a novel, multi-stage architecture designed to produce rich, context-aware embeddings for comic book panels. The architecture is predicated on a hierarchical principle: first deconstructing the panel into its fundamental components, then fusing them into a unified whole, and finally situating this whole within its narrative sequence.

### **3.1 Stage 1: Atomic Panel Representation (Deconstruction & Feature Extraction)**

The foundational step is to create distinct, high-quality vector representations for the different modalities co-existing within a single panel. Instead of collapsing all information into a single, opaque embedding from the outset, the proposed approach leverages a powerful, pre-trained Vision-Language Model (VLM) as a sophisticated feature extractor.12 By using carefully designed prompts, the VLM can be guided to output disentangled vector representations for each modality, preserving their unique characteristics for a more nuanced downstream fusion.43

The proposed model will generate three primary vectors for each panel:

1. **Visual Semantics Vector (Vvis​):** This vector captures the core visual information of the panel. It is derived from the vision encoder component of the VLM (e.g., a Vision Transformer, or ViT) and represents the panel's content (characters, objects, setting), artistic style (line work, coloring, texture), and overall aesthetic. This is analogous to a standard image embedding.  
2. **Textual Semantics Vector (Vtxt​):** This vector represents the complete linguistic content of the panel. All textual elements identified by an upstream OCR process—including dialogue in speech balloons, narrative captions in boxes, and onomatopoeic sound effects—are aggregated into a single string. This string is then fed through the VLM's language model component to produce a deep contextual embedding that captures the explicit textual narrative.  
3. **Compositional Semantics Vector (Vcomp​):** This novel vector is a critical differentiator of the proposed framework, designed to encode the "cinematography" or visual grammar of the panel. Its purpose is to represent how the content is framed and presented to the reader, which is a key aspect of visual storytelling.26 This vector can be generated via one of two methods:  
   * **Structured Encoding:** This method leverages structured data, such as the JSON output from a pre-processing script like batch\_comic\_analysis\_multi.py. Key compositional features are extracted, such as the panel's aspect ratio, the number of characters present, the normalized positions of characters within the frame, and an estimation of the shot scale (e.g., calculated from the ratio of a character's bounding box area to the total panel area).46 These discrete features are then projected into a dense vector representation using a small multi-layer perceptron (MLP).47  
   * **Learned Encoding:** A more powerful, data-driven alternative involves fine-tuning a VLM on a specific task of generating a textual description of a panel's composition (e.g., "A low-angle medium shot of two characters positioned on the left side of the frame, with negative space on the right"). The final hidden state or embedding of this generated descriptive text then serves as the compositional vector, Vcomp​. This approach allows the model to learn a richer, more nuanced representation of composition directly from data.

### **3.2 Stage 2: Intra-Panel Fusion (Creating a Coherent Panel Vector)**

Once the disentangled vectors (Vvis​, Vtxt​, and Vcomp​) are extracted, they must be fused into a single, unified panel embedding, denoted as Pi​ for the i-th panel. The choice of fusion mechanism is critical, as it determines how effectively the model can capture the complex interplay between the modalities.

* **Baseline Method (Concatenation):** The simplest approach is to concatenate the three vectors and pass them through a linear projection layer: Pi​=W⋅\[Vvis​;Vtxt​;Vcomp​\]+b. While computationally efficient, this method is shallow. It treats the modalities as independent blocks of information and fails to model their intricate, co-dependent relationships.  
* **Proposed Approach (Cross-Modal Attention Fusion):** A more sophisticated and powerful methodology involves using a compact Transformer-based fusion module.39 This module employs cross-attention mechanisms to allow the modal representations to dynamically influence one another.40 For instance, the visual vector  
  Vvis​ can serve as the primary sequence of tokens (patches), while the textual vector Vtxt​ and compositional vector Vcomp​ act as context. The model can then learn to attend from the visual features to the textual and compositional features.

This dynamic fusion process enables the model to learn context-sensitive interpretations. Consider these scenarios:

* In a silent panel showing a vast, empty landscape, the model can learn to down-weight the influence of the (null) textual vector Vtxt​.  
* In a "talking heads" panel with two characters in dialogue against a simple background, the model can learn that the meaning is primarily driven by the text, and thus Vtxt​ should heavily modulate the interpretation of the relatively simple Vvis​.  
* In a chaotic fight scene, the model can learn the complex interplay between all three vectors. The onomatopoeia in Vtxt​ (e.g., "KRAKOOM\!") can amplify the importance of visual features in Vvis​ related to an explosion, while the compositional features in Vcomp​ (e.g., a canted angle, rapid panel cuts) can further inform the overall representation of chaos and action.

This learned, dynamic "mixing" of modalities is crucial for creating a panel embedding that is truly representative of its narrative function.

### **3.3 Stage 3: Inter-Panel Encoding (Capturing Narrative Sequence)**

With a sequence of rich, fused panel embeddings \[P1​,P2​,...,PN​\] for a given narrative segment (e.g., a page or a scene), the next stage is to encode their sequential relationships. The goal is to produce a new sequence of context-aware embeddings $$, where each vector Si​ is informed by its surrounding panels and understands its specific place in the narrative flow.

* **Proposed Architecture (Bidirectional Transformer Encoder):** The ideal architecture for this task is a bidirectional Transformer encoder, inspired by the success of models like BERT in natural language processing and ASTERX in comics analysis.24 The sequence of panel embeddings  
  \[P1​,P2​,...,PN​\] serves as the input sequence to the Transformer. The self-attention mechanism within the Transformer allows every panel to attend to every other panel in the sequence, making it exceptionally effective at capturing long-range dependencies—a critical requirement for comic narratives where an event in panel 1 can have repercussions in panel 20\. The final hidden state output from the Transformer corresponding to each input panel Pi​ becomes the final, sequential panel embedding Si​.

The choice of a Transformer is deliberate and justified when compared to other sequential architectures, as summarized in Table 1\.

| Architecture | Key Mechanism | Strengths | Weaknesses | Suitability for Comic Sequences |
| :---- | :---- | :---- | :---- | :---- |
| **RNN (LSTM/GRU)** | Recurrent state updates, processing one element at a time. | Good for short sequences; established history in NLP. | Suffers from vanishing/exploding gradients; difficult to parallelize; struggles with long-range dependencies.50 | **Moderate.** Suitable for short sequences of panels but will likely fail to maintain context across a full page or story arc. |
| **1D CNN** | Sliding convolutional filters over the sequence of embeddings. | Highly parallelizable; computationally efficient; effective at capturing local patterns (e.g., 2-3 panel transitions). | Fixed kernel size limits the range of dependencies it can capture without very deep networks or dilated convolutions. | **Moderate.** Good for capturing local action-to-action transitions but less effective for modeling the overall narrative structure of a page or scene. |
| **Transformer** | Self-attention mechanism, where each element attends to all other elements in the sequence. | Excellent at modeling long-range dependencies; highly parallelizable; state-of-the-art in most sequence tasks.41 | Computationally intensive (quadratic complexity with sequence length); requires large amounts of data for effective training. | **High.** The ability to model long-range dependencies is critical for understanding comic narratives, where an event in panel 1 can be crucial for interpreting panel 20\. Its success in video 30 and text makes it the ideal choice. |

* **Self-Supervised Pre-training Objectives:** To train this sequential encoder effectively on large, unlabeled comic book corpora, a combination of self-supervised learning objectives is proposed. These tasks force the model to learn the underlying narrative grammar of comics without explicit labels.  
  1. **Masked Panel Modeling (MPM):** This task is a direct analogue of Masked Language Modeling (MLM) in BERT and is used in ASTERX.24 A panel embedding  
     Pk​ within a sequence is masked (i.e., replaced with a special \`\` token). The model is then trained to predict the original, unmasked vector Pk​ based on the surrounding context of panels ,Pk+1​,...\]. As the space of panel embeddings is continuous, this is framed as a retrieval task, where the model must select the correct panel embedding from a large set of negative candidates.24  
  2. **Panel Order Prediction (POP):** To learn longer-range relationships, the model is presented with two distinct sequences of panels. It must then perform a classification task to determine if the second sequence correctly follows the first, if their order is swapped, or if they are from completely unrelated parts of the comic. This objective compels the model to understand causality and temporal progression.  
  3. **Reading Path Prediction (RPP):** This novel objective is designed to explicitly teach the model the spatial grammar of the comic page, addressing the limitation identified in the literature review. Using the 2D layout information, the model is given a panel Pi​ and the embeddings of all its adjacent neighbors on the page (above, below, left, right). The task is to predict which of these neighbors is the true next panel in the reading sequence, Pi+1​. This forces the model to learn the visual language of panel layouts as described by Cohn 25, moving beyond simple 1D sequentiality.

## **Section 4: Hierarchical Aggregation for Multi-Scale Understanding**

While panel-level embeddings are essential for granular analysis, many critical tasks—such as genre classification, artist identification, or story arc analysis—require representations of larger narrative units. A simple summation or averaging of panel embeddings would discard the rich sequential and structural information learned in the previous stage. To address this, a hierarchical aggregation framework is proposed, inspired directly by the success of Hierarchical Attention Networks (HANs) in document classification and visual storytelling.34 This framework constructs page-level and comic-level embeddings in a principled, bottom-up fashion.

### **4.1 The Need for Hierarchy**

Comics are inherently hierarchical structures: panels form pages, and pages form a complete narrative.14 An effective computational model should mirror this structure. The proposed aggregation mechanism is designed not only to summarize information but also to learn the relative importance of different components at each level of the hierarchy. This allows the model to identify key moments, turning points, and salient features of the narrative, creating representations that are more than just an average of their parts.

### **4.2 Level 1: From Panels to Page Embedding**

The first level of aggregation combines the sequence of context-aware panel embeddings on a single page into a unified page embedding.

* **Architecture:** This stage employs a "Page Encoder" module, which can be implemented as a GRU or, preferably, a small Transformer-based network, augmented with an attention mechanism.  
* **Process:**  
  1. The sequence of final panel embeddings for a given page, $$, is fed as input to the Page Encoder.  
  2. The encoder processes this sequence to produce a set of hidden states, one for each panel.  
  3. A final attention layer is applied over these hidden states. This layer learns to assign an attention weight, αi​, to each panel on the page. These weights reflect the model's learned assessment of each panel's contribution to the overall meaning or narrative impact of that specific page. For example, a large, visually detailed splash panel might receive a significantly higher attention weight than a small, simple transitional panel.  
  4. The final **Page Embedding (Epage​)** is computed as the attention-weighted sum of the panel hidden states. This vector represents a summary of the page's content and narrative function, with a focus on its most salient elements.

### **4.3 Level 2: From Pages to Comic Embedding**

The second level of aggregation operates analogously to the first, but at a higher scale, combining the sequence of page embeddings into a single vector representing the entire comic book.

* **Architecture:** A "Comic Encoder" module, architecturally identical to the Page Encoder (e.g., a GRU or Transformer with attention), is used.  
* **Process:**  
  1. The sequence of all page embeddings for an entire comic book, \[Epage\_1​,Epage\_2​,...,Epage\_M​\], is provided as input to the Comic Encoder.  
  2. The encoder processes this sequence of pages.  
  3. An attention mechanism is applied over the sequence of pages, learning to assign weights that signify the narrative importance of each page. The model might learn to assign higher weights to pages containing the inciting incident, the climax of the story, or the final resolution.  
  4. The final, comprehensive **Comic-Level Embedding (Ecomic​)** is calculated as the attention-weighted sum of the page representations. This single vector encapsulates the thematic, stylistic, and narrative essence of the entire work.

The attention weights generated by this hierarchical process are not merely an intermediate step in the computation; they are a valuable and interpretable output. By visualizing the attention scores at both the panel and page levels, one can effectively generate an automatic "narrative summary" of a comic. High-weight panels and pages correspond to the moments the model has identified as most critical to the story. This creates a powerful analytical tool for identifying key plot points, character introductions, or climactic scenes without any explicit supervision, emerging directly from the aggregation architecture itself.

## **Section 5: Data Pipeline and Implementation Strategy**

The successful implementation of the proposed embedding framework relies on a robust data pipeline that transforms raw comic book pages and their analyses into a format suitable for the model. This section details the pre-processing steps, leveraging the output of existing analysis scripts, and contrasts the capabilities of the proposed model with current approaches.

### **5.1 Data Pre-processing from batch\_comic\_analysis\_multi.py**

The JSON output from the batch\_comic\_analysis\_multi.py script, which provides detailed structural analysis of a comic page, serves as the primary input for the embedding pipeline. The following steps are required to prepare the data for the model:

1. **Panel Extraction:** For each entry in the JSON file, use the panel\_coords bounding box to crop the original, high-resolution page image. This isolates each panel as a separate image file, which will be the input for the VLM's vision encoder.  
2. **Text Aggregation:** For each panel, iterate through its associated textual elements. Concatenate the text content from the dialogue, narration, and sfx fields into a single, unified string. This string will be the input for the VLM's language model to generate the textual semantics vector (Vtxt​).  
3. **Compositional Feature Extraction:** For each panel, calculate the structured features required for the compositional semantics vector (Vcomp​):  
   * **Aspect Ratio:** Compute from the width and height of the panel\_coords.  
   * **Character Count:** Count the number of dictionaries within the character\_coords list.  
   * **Shot Scale Estimation:** For each character, calculate the area of their bounding box (character\_coords) and divide it by the total area of the panel. This ratio provides a proxy for shot scale (e.g., a high ratio suggests a close-up, a low ratio suggests a long shot). These ratios can be averaged or used as a feature vector.  
   * **Character Positioning:** For each character, normalize their bounding box coordinates by subtracting the panel's top-left corner coordinates and dividing by the panel's dimensions. This yields relative positions within the panel (e.g., \[x\_{rel}, y\_{rel}, w\_{rel}, h\_{rel}\]), which are invariant to the panel's absolute position on the page.  
4. **Sequence Assembly:** Using the panel index provided in the data, assemble the extracted panel images, the aggregated text strings, and the computed compositional features into ordered sequences. Each element in the sequence corresponds to a panel and contains all the necessary data to generate the three initial vectors (Vvis​, Vtxt​, Vcomp​). These sequences are then ready to be batched and fed into the model for training or inference.

### **5.2 Comparison with comic\_search\_with\_captions\_fixed.py**

To understand the leap in capability offered by the proposed framework, it is useful to compare it with a more conventional approach, as likely implemented in a script such as comic\_search\_with\_captions\_fixed.py.

* **Current Approach:** A typical implementation of such a script would likely involve a "shallow" multimodal strategy. It would use a standard VLM (e.g., CLIP) to generate a single embedding for each panel's image and, separately, a text embedding model (or the same VLM's text encoder) to generate an embedding for an associated caption or extracted text. A search query, whether text or image, would be embedded using the same model, and retrieval would be performed by finding the nearest neighbors in the corresponding embedding space (or a simple combination of both spaces). This approach treats each panel as an independent entity and relies solely on content similarity.  
* **Superiority of the Proposed Model:** The proposed hierarchical, sequential embeddings enable far more sophisticated analysis and retrieval capabilities:  
  * **Narrative Awareness:** The core advantage is that the final panel embeddings (Si​) are context-aware. A search for a narrative concept like "a hero's moment of doubt" could retrieve a panel that does not explicitly depict doubt but is narratively positioned immediately after a major failure and before a moment of resolve. The sequential nature of the embedding captures this contextual meaning, something impossible for a system that sees each panel in isolation.  
  * **Compositional Search:** The inclusion of the compositional vector (Vcomp​) allows for entirely new classes of queries that are about the *language* of comics, not just its content. A user could search for "all panels using an extreme close-up during a conversation" or "find pages that break the nine-panel grid." This enables analysis of artistic style and storytelling technique.  
  * **Hierarchical Analysis and Retrieval:** The aggregated page (Epage​) and comic-level (Ecomic​) embeddings unlock high-level analysis. One could perform similarity searches at the book level to "find comics with a narrative pace similar to *Watchmen*" or "find artists whose style resembles Jack Kirby's." Furthermore, one could analyze the trajectory of page embeddings through a story to identify major shifts in tone or plot, enabling queries like "find story arcs where the visual complexity dramatically increases." These high-level thematic, stylistic, and structural queries are fundamentally beyond the reach of any non-sequential, non-hierarchical embedding model.

## **Section 6: A Multi-Faceted Evaluation Framework**

Evaluating the "quality" of narrative embeddings is a non-trivial challenge, as there is no single, universally accepted metric for quantifying narrative understanding or aesthetic representation.52 Standard NLP metrics like BLEU or ROUGE are inapplicable. Therefore, a comprehensive evaluation strategy must be adopted, combining intrinsic probes of the embedding space's structure with a suite of extrinsic, downstream tasks that measure the embeddings' practical utility for solving meaningful problems in comic analysis.21

### **6.2 Intrinsic Evaluation (Sanity Checks)**

Intrinsic evaluations serve to verify that the learned embedding space has a coherent and meaningful structure. These methods provide qualitative and quantitative sanity checks on the model's representations.55

* **Clustering and Visualization:** High-dimensional embedding spaces can be visualized using dimensionality reduction techniques like t-SNE or UMAP. When applied to the panel embeddings (Si​), it is expected that panels with similar characteristics will form distinct clusters. For example, panels drawn by the same artist, panels depicting fight scenes, or panels from the same story arc should group together. This provides a qualitative assessment of the embedding space. For a quantitative measure, clustering algorithms can be applied, and metrics such as the silhouette score or Davies-Bouldin index can be calculated to assess the separation and cohesion of these clusters.55  
* **Vector Analogy Tests:** To probe the semantic relationships learned by the model, a small, curated dataset of analogy questions can be created. This tests if the embedding space has learned consistent vector offsets for specific concepts. An example of such an analogy would be: vector(′BatmanfightingJoker′)−vector(′Batman′)+vector(′Superman′). The resulting vector should be closest in the embedding space to the vector for 'Superman fighting Lex Luthor'. Success on such tasks indicates that the model has captured meaningful and consistent semantic relationships beyond simple co-occurrence.

### **6.3 Extrinsic Evaluation (Downstream Tasks)**

The most robust measure of an embedding's quality is its performance as a feature set for solving practical, downstream tasks.58 The proposed embeddings should be evaluated on a diverse range of tasks that test their capabilities at the panel, page, and comic levels.

* **Task 1: Narrative Similarity Search**  
  * **Setup:** This task directly evaluates the core purpose of the embeddings. Given a query panel, the system should retrieve the top-K most similar panels from a large, diverse library of comics. Similarity should be measured using a distance metric in the embedding space, such as cosine similarity or Euclidean distance.62  
  * **Evaluation:** Due to the subjective nature of "narrative similarity," this task requires human evaluation. A panel of raters would be presented with the query panel and the retrieved results. They would be asked to judge the relevance of each result based on multiple criteria: visual content, artistic style, compositional similarity, and—most importantly—narrative beat or emotional tone. The aggregated judgments can be used to compute ranking metrics like Normalized Discounted Cumulative Gain (nDCG) or Mean Average Precision (mAP), providing a robust measure of search quality.56  
* **Task 2: Genre, Style, and Era Classification**  
  * **Setup:** This task assesses the ability of the aggregated comic-level embeddings (Ecomic​) to capture high-level, book-wide characteristics. The Ecomic​ vectors are used as input features to train a simple classification model (e.g., a Support Vector Machine or a small MLP).  
  * **Evaluation:** The classifier's performance is measured on a labeled dataset for tasks such as predicting the comic's genre (e.g., Superhero, Sci-Fi, Horror, Manga), identifying the artist or writer, or classifying its decade of publication. High performance (measured by standard metrics like Accuracy, Precision, Recall, and F1-score) would demonstrate that the embeddings successfully encode complex stylistic, thematic, and historical information.  
* **Task 3: Story Arc and Tone Shift Detection**  
  * **Setup:** This task evaluates the sequential nature of the panel embeddings (Si​). The sequence of embeddings for a complete story, $$, can be viewed as a trajectory through the high-dimensional vector space. Algorithms for change-point detection can be applied to this trajectory to identify locations where the vector's properties (e.g., direction, magnitude, local variance) shift abruptly. These shifts are hypothesized to correspond to major narrative turning points.  
  * **Evaluation:** The system's detected change-points would be compared against a ground truth of human-annotated story arc boundaries (e.g., marking the "inciting incident," "midpoint," "climax," and "resolution"). Metrics such as precision and recall on detecting these boundaries would directly test the embedding's capacity to model the dynamic structure of a narrative.  
* **Task 4: Character-Specific Analysis**  
  * **Setup:** This task leverages the embeddings for fine-grained character analysis. First, unsupervised clustering can be performed on the panel embeddings to automatically identify all panels that feature a specific character, even across different art styles or costumes. Second, once a character's panels are identified, their associated embeddings can be analyzed in aggregate. For example, one could average the compositional vectors (Vcomp​) for all of a character's panels to determine their typical "shot scale" or framing, or analyze the distribution of their textual vectors (Vtxt​) to understand their linguistic patterns and sentiment.  
  * **Evaluation:** The quality of the character clustering can be evaluated quantitatively using metrics like purity and Normalized Mutual Information (NMI) against a ground-truth dataset. The analysis component is more qualitative, comparing the computational findings (e.g., "Character X is most often shown in close-ups with negative sentiment dialogue") against established character interpretations from literary and fan criticism.

## **Section 7: Conclusion and Future Research Directions**

### **7.1 Summary of Contributions**

This report has outlined a novel and comprehensive framework for creating multi-modal, sequential, and compositional embeddings for comic books. This methodology represents a significant departure from existing approaches by moving beyond simple content-based representations to a more holistic model of narrative. The core contributions of the proposed framework are:

1. **Granular, Multi-Modal Deconstruction:** Instead of a single opaque vector, each panel is represented by three disentangled embeddings for its visual, textual, and—critically—compositional properties.  
2. **Dynamic Modality Fusion:** The use of a cross-modal attention mechanism for fusion allows the model to learn the complex, context-dependent interplay between vision, text, and composition, rather than treating them as static, independent information channels.  
3. **Sophisticated Sequential Encoding:** A Transformer-based architecture, trained with a suite of self-supervised objectives including a novel Reading Path Prediction task, is proposed to capture both long-range narrative dependencies and the unique spatial grammar of the comic page.  
4. **Hierarchical Aggregation:** An attention-based hierarchical structure aggregates panel embeddings into page-level and comic-level representations, enabling multi-scale analysis and the identification of key narrative moments.

Collectively, this framework provides a theoretical and practical blueprint for developing AI systems that can begin to understand comics not as a collection of images, but as the "invisible art" of sequential storytelling.

### **7.2 Future Research Directions**

The development of these advanced narrative embeddings opens up numerous avenues for future research, pushing the boundaries of computational creativity and media analysis.

* **Generative Models for Comic Creation:** The rich, structured embeddings proposed here could serve as a powerful conditioning signal for generative models like GANs or Diffusion Models. A model could be trained to generate a new panel image, text, and layout, conditioned on the embedding of the previous panel, enabling the creation of novel and coherent comic sequences from scratch.  
* **Explicit Reading Path Modeling:** The Reading Path Prediction (RPP) objective could be expanded into a primary task. Future models could be designed to explicitly output a directed graph representing the navigational flow of a complex page layout. This would treat comic page understanding as a graph traversal problem, providing a much deeper understanding of the reader's experience.  
* **Cross-Lingual and Cross-Cultural Analysis:** The comic-level embeddings (Ecomic​) provide a powerful tool for large-scale comparative cultural studies. By training on a diverse corpus of international comics, researchers could use these embeddings to quantitatively analyze and compare the storytelling conventions, visual languages, and narrative structures of different comic traditions, such as American superhero comics, Japanese Manga, and Franco-Belgian *bandes dessinées*.24  
* **Interactive and Co-Creative Storytelling:** These embeddings could form the backbone of interactive narrative systems. A user could potentially manipulate vectors in the embedding space—for example, by "turning up the dial" on the "action" component of a compositional vector or blending a "happy" text vector with a "sad" one—and have the system generate a new panel that reflects these changes. This would transform the system from a passive analysis tool into an active partner in co-creative storytelling.

#### **Works cited**

1. Understanding Comics \- Wikipedia, accessed August 10, 2025, [https://en.wikipedia.org/wiki/Understanding\_Comics](https://en.wikipedia.org/wiki/Understanding_Comics)  
2. Comics as Visual Storytelling | EBSCO Research Starters, accessed August 10, 2025, [https://www.ebsco.com/research-starters/literature-and-writing/comics-visual-storytelling](https://www.ebsco.com/research-starters/literature-and-writing/comics-visual-storytelling)  
3. Data Modelling and Analysis of Sequential Images Used in Comic Books \- SciTePress, accessed August 10, 2025, [https://www.scitepress.org/Papers/2022/120180/120180.pdf](https://www.scitepress.org/Papers/2022/120180/120180.pdf)  
4. Comics in Clinical Practice: A Grounded Theory Exploration of how Sequential Art is Applied to Talking Therapy. John Pollard, accessed August 10, 2025, [https://repository.mdx.ac.uk/download/50e064d3db65e35fc6a21424a08857fe7c28a0baec3066392d91d1ddbcc59bd2/6276576/JAPollard%20thesis.pdf](https://repository.mdx.ac.uk/download/50e064d3db65e35fc6a21424a08857fe7c28a0baec3066392d91d1ddbcc59bd2/6276576/JAPollard%20thesis.pdf)  
5. One missing piece in Vision and Language: A Survey on Comics Understanding \- arXiv, accessed August 10, 2025, [https://arxiv.org/html/2409.09502v1](https://arxiv.org/html/2409.09502v1)  
6. One missing piece in Vision and Language: A Survey on Comics Understanding, accessed August 10, 2025, [https://www.researchgate.net/publication/384075403\_One\_missing\_piece\_in\_Vision\_and\_Language\_A\_Survey\_on\_Comics\_Understanding](https://www.researchgate.net/publication/384075403_One_missing_piece_in_Vision_and_Language_A_Survey_on_Comics_Understanding)  
7. Comics and Multimodal Storytelling (Chapter 6\) \- The Cambridge Companion to Comics, accessed August 10, 2025, [https://www.cambridge.org/core/books/cambridge-companion-to-comics/comics-and-multimodal-storytelling/BBCF21DF1537B11FBBF8AD343BF41EE7](https://www.cambridge.org/core/books/cambridge-companion-to-comics/comics-and-multimodal-storytelling/BBCF21DF1537B11FBBF8AD343BF41EE7)  
8. \[2503.08561\] ComicsPAP: understanding comic strips by picking the correct panel \- arXiv, accessed August 10, 2025, [https://arxiv.org/abs/2503.08561](https://arxiv.org/abs/2503.08561)  
9. ComicsPAP: understanding comic strips by picking the correct panel \- ResearchGate, accessed August 10, 2025, [https://www.researchgate.net/publication/389748978\_ComicsPAP\_understanding\_comic\_strips\_by\_picking\_the\_correct\_panel](https://www.researchgate.net/publication/389748978_ComicsPAP_understanding_comic_strips_by_picking_the_correct_panel)  
10. A Comprehensive Study of Deep Video Action Recognition, accessed August 10, 2025, [https://arxiv.org/abs/2012.06567](https://arxiv.org/abs/2012.06567)  
11. Digital Comics Image Indexing Based on Deep Learning \- MDPI, accessed August 10, 2025, [https://www.mdpi.com/2313-433X/4/7/89](https://www.mdpi.com/2313-433X/4/7/89)  
12. Vision Language Models Explained \- Hugging Face, accessed August 10, 2025, [https://huggingface.co/blog/vlms](https://huggingface.co/blog/vlms)  
13. Computational Approaches to Comics Analysis \- PubMed, accessed August 10, 2025, [https://pubmed.ncbi.nlm.nih.gov/31705626/](https://pubmed.ncbi.nlm.nih.gov/31705626/)  
14. A Deep Learning Pipeline for the Synthesis of Graphic Novels \- Association for Computational Creativity, accessed August 10, 2025, [https://computationalcreativity.net/iccc21/wp-content/uploads/2021/09/ICCC\_2021\_paper\_52.pdf](https://computationalcreativity.net/iccc21/wp-content/uploads/2021/09/ICCC_2021_paper_52.pdf)  
15. (PDF) What do We Expect from Comic Panel Extraction? \- ResearchGate, accessed August 10, 2025, [https://www.researchgate.net/publication/337232656\_What\_do\_We\_Expect\_from\_Comic\_Panel\_Extraction](https://www.researchgate.net/publication/337232656_What_do_We_Expect_from_Comic_Panel_Extraction)  
16. Digital Comics Image Indexing Based on Deep Learning \- ResearchGate, accessed August 10, 2025, [https://www.researchgate.net/publication/326137469\_Digital\_Comics\_Image\_Indexing\_Based\_on\_Deep\_Learning](https://www.researchgate.net/publication/326137469_Digital_Comics_Image_Indexing_Based_on_Deep_Learning)  
17. CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding \- arXiv, accessed August 10, 2025, [https://arxiv.org/html/2407.03550v2](https://arxiv.org/html/2407.03550v2)  
18. Deep Learning for Comic Book Emotion Analysis | Journal of Student Research, accessed August 10, 2025, [https://www.jsr.org/hs/index.php/path/article/view/7960](https://www.jsr.org/hs/index.php/path/article/view/7960)  
19. (PDF) Comic Characters Detection Using Deep Learning \- ResearchGate, accessed August 10, 2025, [https://www.researchgate.net/publication/322780402\_Comic\_Characters\_Detection\_Using\_Deep\_Learning](https://www.researchgate.net/publication/322780402_Comic_Characters_Detection_Using_Deep_Learning)  
20. From Panels to Prose: Generating Literary Narratives from Comics \- arXiv, accessed August 10, 2025, [https://arxiv.org/html/2503.23344v1](https://arxiv.org/html/2503.23344v1)  
21. ComicsPAP: understanding comic strips by picking the correct panel \- arXiv, accessed August 10, 2025, [https://arxiv.org/html/2503.08561v1](https://arxiv.org/html/2503.08561v1)  
22. arxiv.org, accessed August 10, 2025, [https://arxiv.org/html/2403.03719v1](https://arxiv.org/html/2403.03719v1)  
23. CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding \- arXiv, accessed August 10, 2025, [https://arxiv.org/abs/2407.03550](https://arxiv.org/abs/2407.03550)  
24. Drawing Insights: Sequential Representation Learning in Comics \- BMVA Archive, accessed August 10, 2025, [https://bmva-archive.org.uk/bmvc/2024/papers/Paper\_650/paper.pdf](https://bmva-archive.org.uk/bmvc/2024/papers/Paper_650/paper.pdf)  
25. Navigating Comics: An Empirical and Theoretical Approach to Strategies of Reading Comic Page Layouts \- PMC, accessed August 10, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC3629985/](https://pmc.ncbi.nlm.nih.gov/articles/PMC3629985/)  
26. Mastering Panel Analysis in Comic Art \- Number Analytics, accessed August 10, 2025, [https://www.numberanalytics.com/blog/mastering-panel-analysis-comic-art](https://www.numberanalytics.com/blog/mastering-panel-analysis-comic-art)  
27. A Survey on Video Action Recognition in Sports: Datasets, Methods and Applications \- arXiv, accessed August 10, 2025, [https://arxiv.org/abs/2206.01038](https://arxiv.org/abs/2206.01038)  
28. \[2501.13066\] SMART-Vision: Survey of Modern Action Recognition Techniques in Vision, accessed August 10, 2025, [https://arxiv.org/abs/2501.13066](https://arxiv.org/abs/2501.13066)  
29. \[2305.15692\] Deep Neural Networks in Video Human Action Recognition: A Review \- arXiv, accessed August 10, 2025, [https://arxiv.org/abs/2305.15692](https://arxiv.org/abs/2305.15692)  
30. \[2209.05700\] Vision Transformers for Action Recognition: A Survey \- arXiv, accessed August 10, 2025, [https://arxiv.org/abs/2209.05700](https://arxiv.org/abs/2209.05700)  
31. Intelligent Grimm \- Open-ended Visual Storytelling via Latent Diffusion Models, accessed August 10, 2025, [https://openaccess.thecvf.com/content/CVPR2024/papers/Liu\_Intelligent\_Grimm\_-\_Open-ended\_Visual\_Storytelling\_via\_Latent\_Diffusion\_Models\_CVPR\_2024\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Intelligent_Grimm_-_Open-ended_Visual_Storytelling_via_Latent_Diffusion_Models_CVPR_2024_paper.pdf)  
32. \[1909.12401\] A Hierarchical Approach for Visual Storytelling Using Image Description, accessed August 10, 2025, [https://arxiv.org/abs/1909.12401](https://arxiv.org/abs/1909.12401)  
33. Imagine, Reason and Write: Visual Storytelling with Graph Knowledge and Relational Reasoning \- AAAI, accessed August 10, 2025, [https://cdn.aaai.org/ojs/16410/16410-13-19904-1-2-20210518.pdf](https://cdn.aaai.org/ojs/16410/16410-13-19904-1-2-20210518.pdf)  
34. Hierarchical Attention Networks for Document Classification \- CMU School of Computer Science, accessed August 10, 2025, [https://www.cs.cmu.edu/\~./hovy/papers/16HLT-hierarchical-attention-networks.pdf](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)  
35. A Hierarchical Approach for Visual Storytelling Using Image Description \- ResearchGate, accessed August 10, 2025, [https://www.researchgate.net/publication/336132687\_A\_Hierarchical\_Approach\_for\_Visual\_Storytelling\_Using\_Image\_Description](https://www.researchgate.net/publication/336132687_A_Hierarchical_Approach_for_Visual_Storytelling_Using_Image_Description)  
36. A Hierarchical Network with Gated Memory for Visual Storytelling \- The International Conference on Computational Science, accessed August 10, 2025, [https://www.iccs-meeting.org/archive/iccs2021/papers/127430250.pdf](https://www.iccs-meeting.org/archive/iccs2021/papers/127430250.pdf)  
37. Multimodal Sentiment Analysis: A Survey, accessed August 10, 2025, [https://arxiv.org/html/2305.07611](https://arxiv.org/html/2305.07611)  
38. Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey \- arXiv, accessed August 10, 2025, [https://arxiv.org/html/2406.08068v2](https://arxiv.org/html/2406.08068v2)  
39. Full article: An image-text multimodal fusion of deep learning for detecting insulator defects \- Taylor & Francis Online, accessed August 10, 2025, [https://www.tandfonline.com/doi/full/10.1080/17445760.2025.2469512?af=R](https://www.tandfonline.com/doi/full/10.1080/17445760.2025.2469512?af=R)  
40. A CNN-Transformer Approach for Image-Text Multimodal Classification with Cross-Modal Feature Fusion \- ResearchGate, accessed August 10, 2025, [https://www.researchgate.net/publication/389859822\_A\_CNN-Transformer\_Approach\_for\_Image-Text\_Multimodal\_Classification\_with\_Cross-Modal\_Feature\_Fusion](https://www.researchgate.net/publication/389859822_A_CNN-Transformer_Approach_for_Image-Text_Multimodal_Classification_with_Cross-Modal_Feature_Fusion)  
41. Multimodal Learning with Transformers: A Survey \- arXiv, accessed August 10, 2025, [https://arxiv.org/pdf/2206.06488](https://arxiv.org/pdf/2206.06488)  
42. Architectures for Fusion: How Modern AI Systems Integrate Text, Image, Audio, and Video Data | by CryptoRaven | Medium, accessed August 10, 2025, [https://medium.com/@cryptoraven21q/architectures-for-fusion-how-modern-ai-systems-integrate-text-image-audio-and-video-data-3ed314b5aa35](https://medium.com/@cryptoraven21q/architectures-for-fusion-how-modern-ai-systems-integrate-text-image-audio-and-video-data-3ed314b5aa35)  
43. One missing piece in Vision and Language: A Survey on Comics Understanding \- arXiv, accessed August 10, 2025, [https://arxiv.org/html/2409.09502v2](https://arxiv.org/html/2409.09502v2)  
44. Vision-Language Models Bring Comic Panels to Life, Enhancing Accessibility for Visually Impaired Readers \- AZoAi, accessed August 10, 2025, [https://www.azoai.com/news/20240930/Vision-Language-Models-Bring-Comic-Panels-to-Life-Enhancing-Accessibility-for-Visually-Impaired-Readers.aspx](https://www.azoai.com/news/20240930/Vision-Language-Models-Bring-Comic-Panels-to-Life-Enhancing-Accessibility-for-Visually-Impaired-Readers.aspx)  
45. Dynamic Storytelling: Visual Art of Comic Books Explained\! \- YouTube, accessed August 10, 2025, [https://www.youtube.com/watch?v=xi8Ju8WraF0](https://www.youtube.com/watch?v=xi8Ju8WraF0)  
46. Identifying Shot Scale with Artificial Intelligence | by Amos Stailey-Young | Medium, accessed August 10, 2025, [https://medium.com/@amosstaileyyoung/identifying-shot-scale-with-artificial-intelligence-54011062e0bf](https://medium.com/@amosstaileyyoung/identifying-shot-scale-with-artificial-intelligence-54011062e0bf)  
47. Word Embeddings in NLP \- GeeksforGeeks, accessed August 10, 2025, [https://www.geeksforgeeks.org/nlp/word-embeddings-in-nlp/](https://www.geeksforgeeks.org/nlp/word-embeddings-in-nlp/)  
48. Understanding Vectors From a Machine Learning Perspective \- neptune.ai, accessed August 10, 2025, [https://neptune.ai/blog/understanding-vectors-from-a-machine-learning-perspective](https://neptune.ai/blog/understanding-vectors-from-a-machine-learning-perspective)  
49. Image–Text Person Re-Identification with Transformer-Based Modal Fusion \- MDPI, accessed August 10, 2025, [https://www.mdpi.com/2079-9292/14/3/525](https://www.mdpi.com/2079-9292/14/3/525)  
50. Transformer (deep learning architecture) \- Wikipedia, accessed August 10, 2025, [https://en.wikipedia.org/wiki/Transformer\_(deep\_learning\_architecture)](https://en.wikipedia.org/wiki/Transformer_\(deep_learning_architecture\))  
51. Sequence Intent Classification Using Hierarchical Attention Networks \- ISE Developer Blog, accessed August 10, 2025, [https://devblogs.microsoft.com/ise/sequence-intent-classification/](https://devblogs.microsoft.com/ise/sequence-intent-classification/)  
52. Measuring Narrative Visualizations: A Model-Based Evaluation Framework and Validation, accessed August 10, 2025, [https://www.researchgate.net/publication/391793833\_Measuring\_Narrative\_Visualizations\_a\_Model\_Based\_Evaluation\_Framework\_and\_Validation](https://www.researchgate.net/publication/391793833_Measuring_Narrative_Visualizations_a_Model_Based_Evaluation_Framework_and_Validation)  
53. \[Literature Review\] Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition, accessed August 10, 2025, [https://www.themoonlight.io/en/review/not-yet-the-whole-story-evaluating-visual-storytelling-requires-more-than-measuring-coherence-grounding-and-repetition](https://www.themoonlight.io/en/review/not-yet-the-whole-story-evaluating-visual-storytelling-requires-more-than-measuring-coherence-grounding-and-repetition)  
54. Learning to Rank Visual Stories From Human Ranking Data \- ACL Anthology, accessed August 10, 2025, [https://aclanthology.org/2022.acl-long.441/](https://aclanthology.org/2022.acl-long.441/)  
55. How do you evaluate the quality of embeddings? \- Milvus, accessed August 10, 2025, [https://milvus.io/ai-quick-reference/how-do-you-evaluate-the-quality-of-embeddings](https://milvus.io/ai-quick-reference/how-do-you-evaluate-the-quality-of-embeddings)  
56. Mastering RAG: How to Select an Embedding Model \- Galileo AI, accessed August 10, 2025, [https://galileo.ai/blog/mastering-rag-how-to-select-an-embedding-model](https://galileo.ai/blog/mastering-rag-how-to-select-an-embedding-model)  
57. Evaluating Your Embedding Model | by Zilliz \- Medium, accessed August 10, 2025, [https://medium.com/@zilliz\_learn/evaluating-your-embedding-model-30f54e1ef476](https://medium.com/@zilliz_learn/evaluating-your-embedding-model-30f54e1ef476)  
58. CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books-Bohrium, accessed August 10, 2025, [https://www.bohrium.com/paper-details/cosmo-a-multimodal-transformer-for-page-stream-segmentation-in-comic-books/1151942838716465176-108597](https://www.bohrium.com/paper-details/cosmo-a-multimodal-transformer-for-page-stream-segmentation-in-comic-books/1151942838716465176-108597)  
59. Learning across diverse biomedical data modalities and cohorts: Challenges and opportunities for innovation \- PubMed Central, accessed August 10, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10873158/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10873158/)  
60. Story Generation | Papers With Code, accessed August 10, 2025, [https://paperswithcode.com/task/story-generation/latest?page=7\&q=](https://paperswithcode.com/task/story-generation/latest?page=7&q)  
61. Smoke and Mirrors in Causal Downstream Tasks \- OpenReview, accessed August 10, 2025, [https://openreview.net/forum?id=Iq2IAWozNr](https://openreview.net/forum?id=Iq2IAWozNr)  
62. Measuring similarity from embeddings | Machine Learning \- Google for Developers, accessed August 10, 2025, [https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity)  
63. Vector embeddings \- OpenAI API, accessed August 10, 2025, [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)